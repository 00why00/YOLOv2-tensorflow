{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.023674,
     "end_time": "2021-01-14T06:39:22.722758",
     "exception": false,
     "start_time": "2021-01-14T06:39:22.699084",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# YOLO v2\n",
    "## 基于 Tensorflow2.4 的实现\n",
    "## 使用 VOC2007 和 VOC2012 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T06:39:22.776920Z",
     "iopub.status.busy": "2021-01-14T06:39:22.776131Z",
     "iopub.status.idle": "2021-01-14T06:39:24.134000Z",
     "shell.execute_reply": "2021-01-14T06:39:24.133036Z"
    },
    "papermill": {
     "duration": 1.389167,
     "end_time": "2021-01-14T06:39:24.134122",
     "exception": false,
     "start_time": "2021-01-14T06:39:22.744955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir logs\n",
    "!mkdir weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T06:39:24.166800Z",
     "iopub.status.busy": "2021-01-14T06:39:24.166221Z",
     "iopub.status.idle": "2021-01-14T06:39:24.244993Z",
     "shell.execute_reply": "2021-01-14T06:39:24.244458Z"
    },
    "papermill": {
     "duration": 0.096904,
     "end_time": "2021-01-14T06:39:24.245093",
     "exception": false,
     "start_time": "2021-01-14T06:39:24.148189",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导包\n",
    "import os\n",
    "import struct\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T06:39:24.277748Z",
     "iopub.status.busy": "2021-01-14T06:39:24.277206Z",
     "iopub.status.idle": "2021-01-14T06:39:31.022489Z",
     "shell.execute_reply": "2021-01-14T06:39:31.021624Z"
    },
    "papermill": {
     "duration": 6.763249,
     "end_time": "2021-01-14T06:39:31.022599",
     "exception": false,
     "start_time": "2021-01-14T06:39:24.259350",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导包并测试\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013909,
     "end_time": "2021-01-14T06:39:31.051190",
     "exception": false,
     "start_time": "2021-01-14T06:39:31.037281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1、配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T06:39:31.095067Z",
     "iopub.status.busy": "2021-01-14T06:39:31.093845Z",
     "iopub.status.idle": "2021-01-14T06:39:31.096409Z",
     "shell.execute_reply": "2021-01-14T06:39:31.096823Z"
    },
    "papermill": {
     "duration": 0.031551,
     "end_time": "2021-01-14T06:39:31.096923",
     "exception": false,
     "start_time": "2021-01-14T06:39:31.065372",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    参考https://github.com/pjreddie/darknet/blob/master/cfg/yolov2-voc.cfg\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Training\n",
    "        self.IMAGE_W = 416\n",
    "        self.IMAGE_H = 416\n",
    "        self.GRID_W = 13\n",
    "        self.GRID_H = 13\n",
    "        self.ANCHORS_NUM = 5\n",
    "        self.CLASSES_NUM = 20\n",
    "        self.NOOBJECT_LAMBDA = 1\n",
    "        self.OBJECT_LAMBDA = 5\n",
    "        self.CLASS_LAMBDA = 1\n",
    "        self.COORD_LAMBDA = 1\n",
    "        # 论文中提到的K-means算法得到的VOC数据集的5个box的信息\n",
    "        self.ANCHORS = [1.3221, 1.73145, 3.19275, 4.00944, 5.05587, 8.09892, 9.47112, 4.84053, 11.2364, 10.0071]\n",
    "        self.ANCHORS = np.array(self.ANCHORS)\n",
    "        self.ANCHORS = self.ANCHORS.reshape(-1, 2)\n",
    "\n",
    "        self.VOC_NAME_LABEL_CLASS = {\n",
    "            'none': (0, 'Background'),\n",
    "            'aeroplane': (1, 'Vehicle'),\n",
    "            'bicycle': (2, 'Vehicle'),\n",
    "            'bird': (3, 'Animal'),\n",
    "            'boat': (4, 'Vehicle'),\n",
    "            'bottle': (5, 'Indoor'),\n",
    "            'bus': (6, 'Vehicle'),\n",
    "            'car': (7, 'Vehicle'),\n",
    "            'cat': (8, 'Animal'),\n",
    "            'chair': (9, 'Indoor'),\n",
    "            'cow': (10, 'Animal'),\n",
    "            'diningtable': (11, 'Indoor'),\n",
    "            'dog': (12, 'Animal'),\n",
    "            'horse': (13, 'Animal'),\n",
    "            'motorbike': (14, 'Vehicle'),\n",
    "            'person': (15, 'Person'),\n",
    "            'pottedplant': (16, 'Indoor'),\n",
    "            'sheep': (17, 'Animal'),\n",
    "            'sofa': (18, 'Indoor'),\n",
    "            'train': (19, 'Vehicle'),\n",
    "            'tvmonitor': (20, 'Indoor'),\n",
    "        }\n",
    "        self.VOC_NAME_LABEL = {key:v[0] for key,v in self.VOC_NAME_LABEL_CLASS.items()}\n",
    "        self.VOC_LABEL_NAME = {v[0]:key for key,v in self.VOC_NAME_LABEL_CLASS.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014199,
     "end_time": "2021-01-14T06:39:31.125276",
     "exception": false,
     "start_time": "2021-01-14T06:39:31.111077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2、处理和构造数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T06:39:31.177678Z",
     "iopub.status.busy": "2021-01-14T06:39:31.166557Z",
     "iopub.status.idle": "2021-01-14T06:39:31.182874Z",
     "shell.execute_reply": "2021-01-14T06:39:31.182364Z"
    },
    "papermill": {
     "duration": 0.04346,
     "end_time": "2021-01-14T06:39:31.182954",
     "exception": false,
     "start_time": "2021-01-14T06:39:31.139494",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_example_list(path):\n",
    "    \"\"\"\n",
    "    读取训练集或验证集\n",
    "    :param path: 路径\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    with tf.io.gfile.GFile(path) as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip().split(' ')[0] for line in lines]\n",
    "\n",
    "def parse_xml_to_dict(xml):\n",
    "    \"\"\"\n",
    "    递归地将 xml 转化为 字典\n",
    "    :param xml: 通过解析 xml 得到的 lxml.etree 格式\n",
    "    :return: 包含 xml 的字典\n",
    "    \"\"\"\n",
    "    if len(xml) == 0:\n",
    "        return {xml.tag: xml.text}\n",
    "    result = {}\n",
    "    for child in xml:\n",
    "        child_result = parse_xml_to_dict(child)\n",
    "        if child.tag != 'object':\n",
    "            result[child.tag] = child_result[child.tag]\n",
    "        else:\n",
    "            if child.tag not in result:\n",
    "                result[child.tag] = []\n",
    "            result[child.tag].append(child_result[child.tag])\n",
    "    return {xml.tag: result}\n",
    "\n",
    "def transform(x, y, config):\n",
    "    \"\"\"\n",
    "    从数据集中生成一个 batch size 的标签值， 准备在计算损失时和预测值比较\n",
    "    :param x: 一个 batch size 的图片 (batch size, h, w, 3)\n",
    "    :param y: 一个batch size 的 label (batch size, xmin, ymin, xmax, ymax, label)\n",
    "    :param config: 配置\n",
    "    :return: batch\n",
    "        - x : 要预测的图片（batch_size, IMAGE_H, IMAGE_W, 3）\n",
    "        - detector_mask : 是否有 bounding box 在格子内预测（batch, size, GRID_W, GRID_H, anchors_num, 1）\n",
    "        - y_true_anchor_boxes : bounding box 坐标（batch_size, GRID_W, GRID_H, anchors_num, 5）\n",
    "        - y_true_class_hot : 预测类别的 one hot 编码（batch_size, GRID_W, GRID_H, anchors_num, class_num)\n",
    "        - y_true_boxes_all : 标签值（batch_size, max annotation(这里设置为100), 5）\n",
    "    \"\"\"\n",
    "    anchors = config.ANCHORS\n",
    "    anchors_num = anchors.shape[0]\n",
    "    y = y.numpy()\n",
    "    batch_size = y.shape[0]\n",
    "    detector_mask = np.zeros([batch_size, config.GRID_W, config.GRID_H, anchors_num, 1])\n",
    "    y_true_anchor_boxes = np.zeros([batch_size, config.GRID_W, config.GRID_H, anchors_num, 5])\n",
    "    y_true_class_hot = np.zeros([batch_size, config.GRID_W, config.GRID_H, anchors_num, config.CLASSES_NUM])\n",
    "    y_true_boxes_all = np.zeros(y.shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        boxes = y[i]\n",
    "        for j, box in enumerate(boxes):\n",
    "            w = box[2] - box[0]\n",
    "            h = box[3] - box[1]\n",
    "            cx = (box[0] + box[2])/2  # 中心点坐标\n",
    "            cy = (box[1] + box[3])/2\n",
    "\n",
    "            w *= config.GRID_W\n",
    "            h *= config.GRID_H\n",
    "            cx *= config.GRID_W\n",
    "            cy *= config.GRID_H\n",
    "\n",
    "            y_true_boxes_all[i, j] = np.array([cx, cy, w, h, box[4]])\n",
    "            if w * h <= 0:\n",
    "                continue\n",
    "            # 网格 index\n",
    "            cell_col = np.floor(cx).astype(np.int)\n",
    "            cell_row = np.floor(cy).astype(np.int)\n",
    "            # 寻找 IoU 最高的 anchor\n",
    "            anchors_w, anchors_h = anchors[:, 0], anchors[:, 1]\n",
    "            intersect = np.minimum(w, anchors_w) * np.minimum(h, anchors_h)\n",
    "            union = anchors_w * anchors_h + w * h - intersect\n",
    "            iou = intersect / union\n",
    "            anchor_best = np.argmax(iou)\n",
    "\n",
    "            class_index = int(box[4])\n",
    "            y_true_anchor_boxes[i, cell_col, cell_row, anchor_best] = [cx, cy, w, h, class_index]\n",
    "            y_true_class_hot[i, cell_col, cell_row, anchor_best, class_index-1] = 1\n",
    "            detector_mask[i, cell_col, cell_row, anchor_best] = 1\n",
    "\n",
    "    detector_mask = tf.convert_to_tensor(detector_mask, dtype='int64')\n",
    "    y_true_anchor_boxes = tf.convert_to_tensor(y_true_anchor_boxes, dtype='float32')\n",
    "    y_true_boxes_all = tf.convert_to_tensor(y_true_boxes_all, dtype='float32')\n",
    "    y_true_class_hot = tf.convert_to_tensor(y_true_class_hot, dtype='float32')\n",
    "    batch = (x, detector_mask, y_true_anchor_boxes, y_true_class_hot, y_true_boxes_all)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T06:39:31.230144Z",
     "iopub.status.busy": "2021-01-14T06:39:31.224238Z",
     "iopub.status.idle": "2021-01-14T06:39:31.232250Z",
     "shell.execute_reply": "2021-01-14T06:39:31.232651Z"
    },
    "papermill": {
     "duration": 0.035422,
     "end_time": "2021-01-14T06:39:31.232752",
     "exception": false,
     "start_time": "2021-01-14T06:39:31.197330",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 法一：先生成tfrecords，读取时再一个个的batch解析\n",
    "IMAGE_FEATURE_MAP = {\n",
    "    'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/key/sha256': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32), # 如果数据中存放的list长度大于1, 表示数据是不定长的, 使用VarLenFeature解析\n",
    "    'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
    "    'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
    "    'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
    "    'image/object/class/text': tf.io.VarLenFeature(tf.string),\n",
    "    'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n",
    "    'image/object/difficult': tf.io.VarLenFeature(tf.int64),\n",
    "    'image/object/truncated': tf.io.VarLenFeature(tf.int64),\n",
    "    'image/object/view': tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "\n",
    "def parse_example(serialized_example, height, width):\n",
    "    x = tf.io.parse_single_example(serialized_example, IMAGE_FEATURE_MAP)\n",
    "    x_train = tf.image.decode_jpeg(x['image/encoded'], channels=3)\n",
    "    x_train = tf.image.resize(x_train, (height,width))\n",
    "    labels = tf.cast(tf.sparse.to_dense(x['image/object/class/label']), tf.float32)\n",
    "    y_train = tf.stack([tf.sparse.to_dense(x['image/object/bbox/xmin']), # shape: [m]\n",
    "                        tf.sparse.to_dense(x['image/object/bbox/ymin']), # shape: [m]\n",
    "                        tf.sparse.to_dense(x['image/object/bbox/xmax']), # shape: [m]\n",
    "                        tf.sparse.to_dense(x['image/object/bbox/ymax']), # shape: [m]\n",
    "                        labels  # shape: [m]\n",
    "                        ], axis=1) # shape:[m, 5], m是图片中目标的个数, 每张图片的m可能不一样\n",
    "    # 每个图片最多包含100个目标\n",
    "    paddings = [[0, 100 - tf.shape(y_train)[0]], [0, 0]] # 上下左右分别填充0, 100 - tf.shape(y_train)[0], 0, 0\n",
    "    y_train = tf.pad(y_train, paddings)\n",
    "    return x_train, y_train\n",
    "\n",
    "def OB_tfrecord_dataset(dataset_path, batch_size, config, shuffle=False):\n",
    "    files = tf.data.Dataset.list_files(dataset_path)\n",
    "    dataset = files.flat_map(tf.data.TFRecordDataset)\n",
    "    dataset = dataset.map(lambda x:parse_example(x, height=config.IMAGE_H, width=config.IMAGE_W))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=500)\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)  # 提前获取数据存在缓存里来减少gpu因为缺少数据而等待的情况\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T06:39:31.284985Z",
     "iopub.status.busy": "2021-01-14T06:39:31.264057Z",
     "iopub.status.idle": "2021-01-14T06:39:31.287479Z",
     "shell.execute_reply": "2021-01-14T06:39:31.287007Z"
    },
    "papermill": {
     "duration": 0.040384,
     "end_time": "2021-01-14T06:39:31.287563",
     "exception": false,
     "start_time": "2021-01-14T06:39:31.247179",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 法二：即时的从硬盘读取，不用生成tfrecords，不需事先把每张图片的标签读进来\n",
    "def get_imgPath_and_annotations(data_path, years, config,\n",
    "                                image_subdirectory = 'JPEGImages',\n",
    "                                annotations_dir = 'Annotations',\n",
    "                                ignore_difficult_instances = False):\n",
    "    \"\"\"\n",
    "    得到图片的路径和box\n",
    "    :param data_path: 根目录\n",
    "    :param years: 数据集年份 2007/2012\n",
    "    :param config: 配置\n",
    "    :param image_subdirectory: 图片子文件夹\n",
    "    :param annotations_dir: 标签子文件夹\n",
    "    :param ignore_difficult_instances: 忽略复杂数据\n",
    "    :return: 路径 box\n",
    "    \"\"\"\n",
    "    annotations_list = []\n",
    "    for year in years.keys():\n",
    "        sets = years[year]\n",
    "        for _set in sets:\n",
    "            examples_path = os.path.join(data_path, year, 'ImageSets', 'Main', _set + '.txt')\n",
    "            annotations_path = os.path.join(data_path, year, annotations_dir)\n",
    "            examples_list = read_example_list(examples_path)\n",
    "            annotation_list = [os.path.join(annotations_path, example + '.xml') for example in examples_list]\n",
    "            annotations_list += annotation_list\n",
    "\n",
    "    img_names = []\n",
    "    max_obj = 200\n",
    "    annotations = []  # 存放每个图片的box\n",
    "    for path in annotations_list:\n",
    "        with tf.io.gfile.GFile(path, 'r') as f:\n",
    "            xml_str = f.read()\n",
    "        xml = etree.fromstring(xml_str)\n",
    "        data = parse_xml_to_dict(xml)['annotation']\n",
    "        width = int(data['size']['width'])\n",
    "        height = int(data['size']['height'])\n",
    "\n",
    "        boxes = []\n",
    "        if 'object' not in data:\n",
    "            continue\n",
    "        for obj in data['object']:\n",
    "            difficult = bool(int(obj['difficult']))\n",
    "            if ignore_difficult_instances and difficult:\n",
    "                continue\n",
    "            box = np.array([\n",
    "                float(obj['bndbox']['xmin']) / width,\n",
    "                float(obj['bndbox']['ymin']) / height,\n",
    "                float(obj['bndbox']['xmax']) / width,\n",
    "                float(obj['bndbox']['ymax']) / height,\n",
    "                config.VOC_NAME_LABEL[obj['name']]\n",
    "            ])\n",
    "            boxes.append(box)  # 一个图片的box可能有多个\n",
    "        boxes = np.stack(boxes)\n",
    "        annotations.append(boxes)\n",
    "\n",
    "        img_path = os.path.join(data['folder'], image_subdirectory, data['filename'])\n",
    "        img_path = os.path.join(data_dir, img_path)\n",
    "        img_names.append(img_path)\n",
    "    true_boxes = np.zeros([len(img_names), max_obj, 5])\n",
    "    for idx, boxes in enumerate(annotations):\n",
    "        true_boxes[idx, :boxes.shape[0]] = boxes\n",
    "    return img_names, true_boxes\n",
    "\n",
    "def parse_image(filename, true_boxes, img_h, img_w):\n",
    "    \"\"\"\n",
    "    得到图片和 box\n",
    "    :param filename: 图片路径\n",
    "    :param true_boxes: box\n",
    "    :param img_h: 期望图片长\n",
    "    :param img_w: 期望图片宽\n",
    "    :return: 图片 box\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, (img_h, img_w))\n",
    "    return image, true_boxes\n",
    "\n",
    "def OB_tensor_slices_dataset(data_path, years, batch_size, config, shuffle=False):\n",
    "    img_names, boxes = get_imgPath_and_annotations(data_path, years, config)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_names, boxes))\n",
    "    dataset = dataset.map(lambda x, y:parse_image(x, y, img_h=config.IMAGE_H, img_w=config.IMAGE_W))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=500)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)  # 提前获取数据存在缓存里来减少gpu因为缺少数据而等待的情况\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014593,
     "end_time": "2021-01-14T06:39:31.316665",
     "exception": false,
     "start_time": "2021-01-14T06:39:31.302072",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### 3、定义 YOLO 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T06:39:31.389346Z",
     "iopub.status.busy": "2021-01-14T06:39:31.373810Z",
     "iopub.status.idle": "2021-01-14T06:39:31.404427Z",
     "shell.execute_reply": "2021-01-14T06:39:31.403867Z"
    },
    "papermill": {
     "duration": 0.073349,
     "end_time": "2021-01-14T06:39:31.404512",
     "exception": false,
     "start_time": "2021-01-14T06:39:31.331163",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def space_to_depth(x):\n",
    "    return tf.nn.space_to_depth(x, block_size=2)\n",
    "\n",
    "def yolo(config):\n",
    "    \"\"\"\n",
    "    训练模型，即去掉最后一个卷积层，转而增加了三个3*3*1024的卷积层,集体参看yolov2-voc.cfg文件\n",
    "    :param config: 配置文件\n",
    "    :return: 网络模型\n",
    "    \"\"\"\n",
    "    input_image = tf.keras.layers.Input((config.IMAGE_H, config.IMAGE_W, 3), dtype='float32')\n",
    "    # 1\n",
    "    x = Conv2D(32, (3, 3), strides=(1, 1), padding='same', name='conv_1', use_bias=False)(input_image)\n",
    "    x = BatchNormalization(name='norm_1')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    # 2\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', name='conv_2', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_2')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    # 3\n",
    "    x = Conv2D(128, (3, 3), strides=(1, 1), padding='same', name='conv_3', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_3')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 4\n",
    "    x = Conv2D(64, (1, 1), strides=(1, 1), padding='same', name='conv_4', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_4')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 5\n",
    "    x = Conv2D(128, (3, 3), strides=(1, 1), padding='same', name='conv_5', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_5')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    # 6\n",
    "    x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_6')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 7\n",
    "    x = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_7')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 8\n",
    "    x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_8')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # 9\n",
    "    x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_9', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_9')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 10\n",
    "    x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_10', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_10')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 11\n",
    "    x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_11')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 12\n",
    "    x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_12', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_12')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 13\n",
    "    x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_13')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    skip_connection = x\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # 14\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_14', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_14')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 15\n",
    "    x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_15', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_15')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 16\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_16', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_16')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 17\n",
    "    x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_17', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_17')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 18\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_18')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 19 把最后一层卷积层移除,添加3个1024*3*3卷积层\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_19', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_19')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 20\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_20', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_20')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 21 passthrough层\n",
    "    skip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\n",
    "    skip_connection = BatchNormalization(name='norm_21')(skip_connection)\n",
    "    skip_connection = LeakyReLU(alpha=0.1)(skip_connection)\n",
    "    skip_connection = Lambda(space_to_depth)(skip_connection)\n",
    "    x = concatenate([skip_connection, x])\n",
    "    # 22\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_22', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_22')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    # 23 1*1的卷积层生成预测输出\n",
    "    x = Conv2D(config.ANCHORS_NUM * (4 + 1 + config.CLASSES_NUM), (1,1), strides=(1,1), padding='same', name='conv_23')(x)\n",
    "    output = Reshape((config.GRID_W, config.GRID_H, config.ANCHORS_NUM, 4 + 1 + config.CLASSES_NUM))(x)\n",
    "\n",
    "    yolo_model = keras.models.Model(input_image, output)\n",
    "    return yolo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01547,
     "end_time": "2021-01-14T06:39:31.435948",
     "exception": false,
     "start_time": "2021-01-14T06:39:31.420478",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### 4、加载预训练权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T06:39:31.491653Z",
     "iopub.status.busy": "2021-01-14T06:39:31.473173Z",
     "iopub.status.idle": "2021-01-14T06:39:31.494396Z",
     "shell.execute_reply": "2021-01-14T06:39:31.493945Z"
    },
    "papermill": {
     "duration": 0.043578,
     "end_time": "2021-01-14T06:39:31.494478",
     "exception": false,
     "start_time": "2021-01-14T06:39:31.450900",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WeightReader:\n",
    "    def __init__(self, weight_file):\n",
    "        with open(weight_file, 'rb') as w_f:\n",
    "            major, = struct.unpack('i', w_f.read(4))\n",
    "            minor, = struct.unpack('i', w_f.read(4))\n",
    "            revision, = struct.unpack('i', w_f.read(4))\n",
    "            if (major*10+minor) >=2 and major < 1000 and minor < 1000:\n",
    "                w_f.read(8)\n",
    "            else:\n",
    "                w_f.read(4)\n",
    "            transpose = (major > 1000) or (minor > 1000)\n",
    "            binary = w_f.read()\n",
    "        self.offset = 0\n",
    "        self.all_weights = np.frombuffer(binary, dtype='float32')\n",
    "\n",
    "    def read_bytes(self, size):\n",
    "        self.offset = self.offset + size\n",
    "        return self.all_weights[self.offset-size:self.offset]\n",
    "\n",
    "    def load_weights(self, yolo_model, conv_num=23, if_last=False):\n",
    "        conv_num_read = conv_num\n",
    "        if not if_last:\n",
    "            conv_num_read = conv_num - 1\n",
    "\n",
    "        for i in range(1, conv_num + 1):\n",
    "            try:\n",
    "                conv_layer = yolo_model.get_layer('conv_' + str(i))\n",
    "                if i < conv_num:\n",
    "                    norm_layer = yolo_model.get_layer('norm_' + str(i))\n",
    "                    size = np.prod(norm_layer.get_weights()[0].shape)\n",
    "                    beta = self.read_bytes(size)  # bias\n",
    "                    gamma = self.read_bytes(size)  # scale\n",
    "                    mean = self.read_bytes(size)  # mean\n",
    "                    var = self.read_bytes(size)  # variance\n",
    "                    weights = norm_layer.set_weights([gamma, beta, mean, var])\n",
    "                if len(conv_layer.get_weights()) > 1:\n",
    "                    bias = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n",
    "                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "                    kernel = kernel.transpose([2, 3, 1, 0])\n",
    "                    conv_layer.set_weights([kernel, bias])\n",
    "                else:\n",
    "                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "                    kernel = kernel.transpose([2,3,1,0])\n",
    "                    conv_layer.set_weights([kernel])\n",
    "            except ValueError:\n",
    "                print(\"no convolution #\" + str(i))\n",
    "\n",
    "        if not if_last:\n",
    "          layer = yolo_model.layers[-2] # last convolutional layer\n",
    "          layer.trainable = True\n",
    "          weights = layer.get_weights()\n",
    "          new_kernel = np.random.normal(size=weights[0].shape)/(13*13)\n",
    "          new_bias = np.random.normal(size=weights[1].shape)/(13*13)\n",
    "          layer.set_weights([new_kernel, new_bias])\n",
    "\n",
    "    def reset(self):\n",
    "        self.offset = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014591,
     "end_time": "2021-01-14T06:39:31.524404",
     "exception": false,
     "start_time": "2021-01-14T06:39:31.509813",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### 5、IoU 和 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T06:39:31.583536Z",
     "iopub.status.busy": "2021-01-14T06:39:31.561499Z",
     "iopub.status.idle": "2021-01-14T06:39:31.594403Z",
     "shell.execute_reply": "2021-01-14T06:39:31.593967Z"
    },
    "papermill": {
     "duration": 0.055385,
     "end_time": "2021-01-14T06:39:31.594489",
     "exception": false,
     "start_time": "2021-01-14T06:39:31.539104",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cal_iou(x1, y1, w1, h1, x2, y2, w2, h2):\n",
    "    \"\"\"\n",
    "    计算 box1 和 box2 的 IoU\n",
    "    \"\"\"\n",
    "    xmin1 = x1 - 0.5 * w1\n",
    "    xmax1 = x1 + 0.5 * w1\n",
    "    ymin1 = y1 - 0.5 * h1\n",
    "    ymax1 = y1 + 0.5 * h1\n",
    "    xmin2 = x2 - 0.5 * w2\n",
    "    xmax2 = x2 + 0.5 * w2\n",
    "    ymin2 = y2 - 0.5 * h2\n",
    "    ymax2 = y2 + 0.5 * h2\n",
    "    inter_x = np.minimum(xmax1, xmax2) - np.maximum(xmin1, xmin2)\n",
    "    inter_y = np.minimum(ymax1, ymax2) - np.maximum(ymin1, ymin2)\n",
    "    inter = inter_x * inter_y\n",
    "    union = w1 * h1 + w2 * h2 - inter\n",
    "    iou = inter / (union + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def yolo_loss(detector_mask, y_true_anchor_boxes, y_true_class_hot, y_true_boxes_all, y_pred, config):\n",
    "    \"\"\"\n",
    "    损失函数\n",
    "    :param detector_mask: shape(batch_size, GRID_W, GRID_H, anchors_count, 1)\n",
    "    :param y_true_anchor_boxes: shape(batch_size, GRID_W, GRID_H, anchors_count, 5)\n",
    "    :param y_true_class_hot: one hot编码 shape(batch_size, GRID_W, GRID_H, anchors_count, class_count)\n",
    "    :param y_true_boxes_all: (x, y, w, h, c)\n",
    "    :param y_pred: shape(batch_size, GRID_W, GRID_H, anchors_count, 5 + labels count)\n",
    "    :param config: 配置\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "    # 0-GRID_W -1 / GRID_H -1\n",
    "    cell_coord_x = tf.cast(tf.reshape(tf.tile(tf.range(config.GRID_W), [config.GRID_H]), (1, config.GRID_H, config.GRID_W, 1, 1)), tf.float32)\n",
    "    cell_coord_y = tf.transpose(cell_coord_x, (0,2,1,3,4))\n",
    "    cell_coords = tf.tile(tf.concat([cell_coord_x, cell_coord_y], -1), [y_pred.shape[0], 1, 1, 5, 1])\n",
    "\n",
    "    # 0-GRID_W / GRID_H\n",
    "    anchors = config.ANCHORS\n",
    "\n",
    "    # 0-GRID_W / GRID_H\n",
    "    pred_xy = K.sigmoid(y_pred[:,:,:,:,0:2])\n",
    "    pred_xy = pred_xy + cell_coords\n",
    "    pred_wh = K.exp(y_pred[:,:,:,:,2:4]) * anchors\n",
    "\n",
    "    # 1、坐标损失（coordinate loss）\n",
    "    # 根据gt的wh计算系数，系数作用是w和h值越小，损失系数越大，可以更好地学习尺度较小的box\n",
    "    lambda_wh = K.expand_dims(2-(y_true_anchor_boxes[:,:,:,:,2]/config.GRID_W) * (y_true_anchor_boxes[:,:,:,:,3]/config.GRID_H))\n",
    "    detector_mask = K.cast(detector_mask, tf.float32)  # batch_size, GRID_W, GRID_H, n_anchors, 1\n",
    "    n_objs = K.sum(K.cast(detector_mask>0, tf.float32))\n",
    "    # 基于预测值计算坐标损失\n",
    "    y_txy = y_true_anchor_boxes[...,0:2] - cell_coords\n",
    "    y_twh = K.log(y_true_anchor_boxes[...,2:4]*1.0/anchors + 1e-16)\n",
    "    pred_txy = K.sigmoid(y_pred[:,:,:,:,0:2])\n",
    "    pred_twh = y_pred[:,:,:,:,2:4]\n",
    "    loss_xy = config.COORD_LAMBDA * K.sum(detector_mask * K.square(y_txy - pred_txy)) / (n_objs + 1e-6)\n",
    "    loss_wh = config.COORD_LAMBDA * K.sum(lambda_wh * detector_mask * K.square(y_twh - pred_twh)) / (n_objs + 1e-6)\n",
    "    loss_coord = loss_xy + loss_wh\n",
    "\n",
    "    # 2、类别损失\n",
    "    pred_class = K.softmax(y_pred[:,:,:,:,5:])\n",
    "    loss_cls = detector_mask * K.square(y_true_class_hot - pred_class)\n",
    "    loss_cls = config.CLASS_LAMBDA * K.sum(loss_cls) / (n_objs + 1e-6)\n",
    "\n",
    "    # 3、bounding box置信度损失\n",
    "    # 3.1、包含目标的预测的bounding box置信度损失\n",
    "    # 预测值和标记值的 IoU\n",
    "    x1 = y_true_anchor_boxes[...,0]\n",
    "    y1 = y_true_anchor_boxes[...,1]\n",
    "    w1 = y_true_anchor_boxes[...,2]\n",
    "    h1 = y_true_anchor_boxes[...,3]\n",
    "    x2 = pred_xy[...,0]\n",
    "    y2 = pred_xy[...,1]\n",
    "    w2 = pred_wh[...,0]\n",
    "    h2 = pred_wh[...,1]\n",
    "    iou = cal_iou(x1, y1, w1, h1, x2, y2, w2, h2)\n",
    "    iou = K.expand_dims(iou, -1)\n",
    "    # 使用 IoU 计算置信度的 target\n",
    "    pred_conf = K.sigmoid(y_pred[...,4:5])\n",
    "    loss_conf_obj = config.OBJECT_LAMBDA * K.sum(detector_mask * K.square(iou - pred_conf)) / (n_objs + 1e-6)\n",
    "    # 3.2、不包含目标的预测的bounding box置信度损失\n",
    "    # 预测值bounding box的xmin, ymin, xmax, ymax\n",
    "    pred_xy = K.expand_dims(pred_xy, 4)  # shape(batch_size, GRID_W, GRID_H, n_anchors, 1, 2)\n",
    "    pred_wh = K.expand_dims(pred_wh, 4)\n",
    "    pred_wh_half = pred_wh / 2.\n",
    "    pred_min = pred_xy - pred_wh_half\n",
    "    pred_max = pred_xy + pred_wh_half\n",
    "    # 标记值bounding box的xmin, ymin, xmax, ymax\n",
    "    true_boxes_shape = K.int_shape(y_true_boxes_all)\n",
    "    true_boxes_grid = K.reshape(y_true_boxes_all, [true_boxes_shape[0], 1, 1, 1, true_boxes_shape[1], true_boxes_shape[2]])\n",
    "    true_xy = true_boxes_grid[...,0:2]  # shape(batch_size, 1, 1, 1, max_annotation, 2)\n",
    "    true_wh = true_boxes_grid[...,2:4]  # shape(batch_size, 1, 1, 1, max_annotation, 2)\n",
    "    true_wh_half = true_wh * 0.5\n",
    "    true_min = true_xy - true_wh_half\n",
    "    true_max = true_xy + true_wh_half\n",
    "    # 计算每一个预测的box与所有标记的box的IoU，找出最大的IOU，如果小于阈值(0.6,并且不负责GT，根据1 - detector_mask)，该预测的box就加入noobj，计算置信度损失\n",
    "    intersect_min = K.maximum(pred_min, true_min)  # shape(batch_size, GRID_W, GRID_H, n_anchors, max_annotation, 2)\n",
    "    intersect_max = K.minimum(pred_max, true_max)  # shape(batch_size, GRID_W, GRID_H, n_anchors, max_annotation, 2)\n",
    "    intersect_wh = K.maximum(intersect_max - intersect_min, 0.)  # shape(batch_size, GRID_W, GRID_H, n_anchors, max_annotation, 1)\n",
    "    intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]  # shape(batch_size, GRID_W, GRID_H, n_anchors, max_annotation, 1)\n",
    "    pred_area = pred_wh[..., 0] * pred_wh[..., 1]  # shape(batch_size, GRID_W, GRID_H, n_anchors, 1, 1)\n",
    "    true_area = true_wh[..., 0] * true_wh[..., 1]  # shape(batch_size, GRID_W, GRID_H, n_anchors, max_annotation, 1)\n",
    "    union_area = pred_area + true_area - intersect_area\n",
    "    iou_score = intersect_area / union_area  # shape(batch_size, GRID_W, GRID_H, n_anchors, max_annotation, 1)\n",
    "    best_iou = K.max(iou_score, axis=4)  # Best IOU scores.\n",
    "    best_iou = K.expand_dims(best_iou)  # shape(batch_size, GRID_W, GRID_H, n_anchors, 1)\n",
    "    # 计算无目标损失\n",
    "    no_object_detection = K.cast(best_iou < 0.6, K.dtype(best_iou))\n",
    "    noobj_mask = no_object_detection * (1 - detector_mask)\n",
    "    n_noobj  = K.sum(tf.cast(noobj_mask  > 0.0, tf.float32))\n",
    "    loss_conf_noobj =  config.NOOBJECT_LAMBDA * K.sum(noobj_mask * K.square(-pred_conf)) / (n_noobj + 1e-6)\n",
    "    # 4、三种损失汇总\n",
    "    loss_conf = loss_conf_noobj + loss_conf_obj\n",
    "    loss = loss_conf + loss_cls + loss_coord\n",
    "    sub_loss = [loss_conf, loss_cls, loss_coord]\n",
    "    return loss, sub_loss\n",
    "\n",
    "def save_best_weights(model, name, val_loss_avg):\n",
    "    \"\"\"\n",
    "    保存最好的权重\n",
    "    \"\"\"\n",
    "    name = name + '_' + str(val_loss_avg) + '.h5'\n",
    "    path_name = os.path.join('weight/', name)\n",
    "    model.save_weights(path_name)\n",
    "    return path_name\n",
    "\n",
    "def log_loss(loss, val_loss, step):\n",
    "    tf.summary.scalar('loss', loss, step)\n",
    "    tf.summary.scalar('val_loss', val_loss, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014862,
     "end_time": "2021-01-14T06:39:31.624478",
     "exception": false,
     "start_time": "2021-01-14T06:39:31.609616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 6、开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T06:39:31.663084Z",
     "iopub.status.busy": "2021-01-14T06:39:31.662477Z",
     "iopub.status.idle": "2021-01-14T06:40:38.198221Z",
     "shell.execute_reply": "2021-01-14T06:40:38.197720Z"
    },
    "papermill": {
     "duration": 66.558751,
     "end_time": "2021-01-14T06:40:38.198340",
     "exception": false,
     "start_time": "2021-01-14T06:39:31.639589",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "data_dir = '../input/voc-2007-2012/VOCdevkit'\n",
    "val_set = {'VOC2012':['val']}\n",
    "train_set = {'VOC2007':['train', 'val', 'test'], 'VOC2012':['train']}\n",
    "\n",
    "batch_size = 6\n",
    "num_epochs = 200\n",
    "num_iterations = 30\n",
    "train_name = 'training_1'\n",
    "cfg = Config()\n",
    "model_weight_path = '../input/voc-2007-2012/weight/yolo.weights'\n",
    "n_progress = 20  # 进度条份数\n",
    "\n",
    "# log（tensorboard）\n",
    "summery_writer = tf.summary.create_file_writer(os.path.join('logs/', train_name), flush_millis=20000)\n",
    "summery_writer.set_as_default()\n",
    "\n",
    "dataset_train = OB_tensor_slices_dataset(data_dir, train_set, batch_size, cfg, shuffle=True)\n",
    "dataset_val = OB_tensor_slices_dataset(data_dir, val_set, batch_size, cfg, shuffle=False)\n",
    "len_batches_train = tf.data.experimental.cardinality(dataset_train).numpy()\n",
    "len_batches_val = tf.data.experimental.cardinality(dataset_val).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T06:40:38.234257Z",
     "iopub.status.busy": "2021-01-14T06:40:38.233669Z",
     "iopub.status.idle": "2021-01-14T06:40:44.123397Z",
     "shell.execute_reply": "2021-01-14T06:40:44.124098Z"
    },
    "papermill": {
     "duration": 5.909421,
     "end_time": "2021-01-14T06:40:44.124295",
     "exception": false,
     "start_time": "2021-01-14T06:40:38.214874",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 416, 416, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 416, 416, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm_1 (BatchNormalization)     (None, 416, 416, 32) 128         conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 416, 416, 32) 0           norm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 208, 208, 32) 0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 208, 208, 64) 18432       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_2 (BatchNormalization)     (None, 208, 208, 64) 256         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 208, 208, 64) 0           norm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 104, 104, 64) 0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, 104, 104, 128 73728       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_3 (BatchNormalization)     (None, 104, 104, 128 512         conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 104, 104, 128 0           norm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, 104, 104, 64) 8192        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_4 (BatchNormalization)     (None, 104, 104, 64) 256         conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 104, 104, 64) 0           norm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_5 (Conv2D)                 (None, 104, 104, 128 73728       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_5 (BatchNormalization)     (None, 104, 104, 128 512         conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 104, 104, 128 0           norm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 52, 52, 128)  0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_6 (Conv2D)                 (None, 52, 52, 256)  294912      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_6 (BatchNormalization)     (None, 52, 52, 256)  1024        conv_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 52, 52, 256)  0           norm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_7 (Conv2D)                 (None, 52, 52, 128)  32768       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_7 (BatchNormalization)     (None, 52, 52, 128)  512         conv_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 52, 52, 128)  0           norm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_8 (Conv2D)                 (None, 52, 52, 256)  294912      leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_8 (BatchNormalization)     (None, 52, 52, 256)  1024        conv_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 52, 52, 256)  0           norm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 26, 26, 256)  0           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_9 (Conv2D)                 (None, 26, 26, 512)  1179648     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_9 (BatchNormalization)     (None, 26, 26, 512)  2048        conv_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 26, 26, 512)  0           norm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_10 (Conv2D)                (None, 26, 26, 256)  131072      leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_10 (BatchNormalization)    (None, 26, 26, 256)  1024        conv_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 26, 26, 256)  0           norm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_11 (Conv2D)                (None, 26, 26, 512)  1179648     leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_11 (BatchNormalization)    (None, 26, 26, 512)  2048        conv_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 26, 26, 512)  0           norm_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_12 (Conv2D)                (None, 26, 26, 256)  131072      leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_12 (BatchNormalization)    (None, 26, 26, 256)  1024        conv_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 26, 26, 256)  0           norm_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_13 (Conv2D)                (None, 26, 26, 512)  1179648     leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_13 (BatchNormalization)    (None, 26, 26, 512)  2048        conv_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 26, 26, 512)  0           norm_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 13, 13, 512)  0           leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_14 (Conv2D)                (None, 13, 13, 1024) 4718592     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_14 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_15 (Conv2D)                (None, 13, 13, 512)  524288      leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_15 (BatchNormalization)    (None, 13, 13, 512)  2048        conv_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 13, 13, 512)  0           norm_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_16 (Conv2D)                (None, 13, 13, 1024) 4718592     leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_16 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_17 (Conv2D)                (None, 13, 13, 512)  524288      leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_17 (BatchNormalization)    (None, 13, 13, 512)  2048        conv_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 13, 13, 512)  0           norm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_18 (Conv2D)                (None, 13, 13, 1024) 4718592     leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_18 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_19 (Conv2D)                (None, 13, 13, 1024) 9437184     leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_19 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_21 (Conv2D)                (None, 26, 26, 64)   32768       leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm_21 (BatchNormalization)    (None, 26, 26, 64)   256         conv_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_20 (Conv2D)                (None, 13, 13, 1024) 9437184     leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 26, 26, 64)   0           norm_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm_20 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 13, 13, 256)  0           leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 13, 13, 1280) 0           lambda[0][0]                     \n",
      "                                                                 leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_22 (Conv2D)                (None, 13, 13, 1024) 11796480    concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "norm_22 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 13, 13, 1024) 0           leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_23 (Conv2D)                (None, 13, 13, 125)  128125      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 13, 13, 5, 25 0           conv_23[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 50,676,061\n",
      "Trainable params: 50,655,389\n",
      "Non-trainable params: 20,672\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 准备模型\n",
    "weight_reader = WeightReader(model_weight_path)\n",
    "model = yolo(cfg)\n",
    "weight_reader.load_weights(model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T06:40:44.183672Z",
     "iopub.status.busy": "2021-01-14T06:40:44.182980Z",
     "iopub.status.idle": "2021-01-14T06:40:44.186760Z",
     "shell.execute_reply": "2021-01-14T06:40:44.186325Z"
    },
    "papermill": {
     "duration": 0.027341,
     "end_time": "2021-01-14T06:40:44.186860",
     "exception": false,
     "start_time": "2021-01-14T06:40:44.159519",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 准备训练\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "best_val_loss = 1e6\n",
    "initial_learning_rate = 2e-5\n",
    "decay_epochs = 30 * num_iterations\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=decay_epochs,\n",
    "    decay_rate=0.5,\n",
    "    staircase=True\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "train_layers = ['conv_22', 'norm_22', 'conv_23']\n",
    "train_vars = []\n",
    "for name in train_layers:\n",
    "     train_vars += model.get_layer(name).trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T06:40:44.236610Z",
     "iopub.status.busy": "2021-01-14T06:40:44.231395Z",
     "iopub.status.idle": "2021-01-14T07:01:12.179426Z",
     "shell.execute_reply": "2021-01-14T07:01:12.178921Z"
    },
    "papermill": {
     "duration": 1227.976646,
     "end_time": "2021-01-14T07:01:12.179587",
     "exception": false,
     "start_time": "2021-01-14T06:40:44.202941",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 :\n",
      "----------------\n",
      "find better model for train\n",
      " \n",
      "train_loss=172.895 (conf=142.315, class=0.954, coords=29.626), val_loss=29.923 (conf=3.964, class=0.949, coords=25.010)\n",
      "\n",
      "Epoch 1 :\n",
      "---------------- \n",
      "train_loss=76252.180 (conf=76223.266, class=0.950, coords=27.955), val_loss=29.882 (conf=3.903, class=0.948, coords=25.031)\n",
      "\n",
      "Epoch 2 :\n",
      "---------------- \n",
      "train_loss=1062.631 (conf=1033.927, class=0.948, coords=27.756), val_loss=29.820 (conf=3.818, class=0.947, coords=25.055)\n",
      "\n",
      "Epoch 3 :\n",
      "----------------\n",
      "find better model for train\n",
      " \n",
      "train_loss=65.492 (conf=36.872, class=0.957, coords=27.663), val_loss=29.825 (conf=3.778, class=0.947, coords=25.100)\n",
      "\n",
      "Epoch 4 :\n",
      "---------------- \n",
      "train_loss=329.256 (conf=298.381, class=0.945, coords=29.929), val_loss=29.790 (conf=3.712, class=0.947, coords=25.132)\n",
      "\n",
      "Epoch 5 :\n",
      "---------------- \n",
      "train_loss=140.513 (conf=110.448, class=0.946, coords=29.118), val_loss=29.769 (conf=3.672, class=0.946, coords=25.151)\n",
      "\n",
      "Epoch 6 :\n",
      "----------------\n",
      "find better model for train\n",
      " \n",
      "train_loss=63.192 (conf=36.277, class=0.945, coords=25.971), val_loss=29.746 (conf=3.635, class=0.946, coords=25.165)\n",
      "\n",
      "Epoch 7 :\n",
      "----------------\n",
      "find better model for train\n",
      " \n",
      "train_loss=56.767 (conf=27.222, class=0.941, coords=28.604), val_loss=29.710 (conf=3.598, class=0.946, coords=25.166)\n",
      "\n",
      "Epoch 8 :\n",
      "---------------- \n",
      "train_loss=819.808 (conf=794.287, class=0.944, coords=24.577), val_loss=29.656 (conf=3.539, class=0.947, coords=25.170)\n",
      "\n",
      "Epoch 9 :\n",
      "----------------\n",
      "find better model for train\n",
      " \n",
      "train_loss=42.962 (conf=13.828, class=0.938, coords=28.197), val_loss=29.630 (conf=3.502, class=0.947, coords=25.181)\n",
      "\n",
      "Epoch 10 :\n",
      "---------------- \n",
      "train_loss=143474.656 (conf=143448.094, class=0.926, coords=25.600), val_loss=29.675 (conf=3.481, class=0.947, coords=25.248)\n",
      "\n",
      "Epoch 11 :\n",
      "---------------- \n",
      "train_loss=89.509 (conf=57.138, class=0.925, coords=31.445), val_loss=29.674 (conf=3.467, class=0.947, coords=25.260)\n",
      "\n",
      "Epoch 12 :\n",
      "---------------- \n",
      "train_loss=124.413 (conf=96.612, class=0.932, coords=26.869), val_loss=29.625 (conf=3.440, class=0.947, coords=25.238)\n",
      "\n",
      "Epoch 13 :\n",
      "---------------- \n",
      "train_loss=44.374 (conf=15.857, class=0.920, coords=27.597), val_loss=29.581 (conf=3.413, class=0.949, coords=25.219)\n",
      "\n",
      "Epoch 14 :\n",
      "---------------- \n",
      "train_loss=56.026 (conf=29.418, class=0.927, coords=25.681), val_loss=29.557 (conf=3.399, class=0.948, coords=25.209)\n",
      "\n",
      "Epoch 15 :\n",
      "---------------- \n",
      "train_loss=45.835 (conf=19.077, class=0.917, coords=25.841), val_loss=29.522 (conf=3.373, class=0.949, coords=25.200)\n",
      "\n",
      "Epoch 16 :\n",
      "---------------- \n",
      "train_loss=1039064.250 (conf=1039036.125, class=0.913, coords=27.108), val_loss=29.672 (conf=3.481, class=0.951, coords=25.240)\n",
      "\n",
      "Epoch 17 :\n",
      "---------------- \n",
      "train_loss=46.448 (conf=17.070, class=0.908, coords=28.469), val_loss=29.796 (conf=3.543, class=0.953, coords=25.300)\n",
      "\n",
      "Epoch 18 :\n",
      "---------------- \n",
      "train_loss=223.143 (conf=194.359, class=0.906, coords=27.878), val_loss=29.717 (conf=3.472, class=0.955, coords=25.290)\n",
      "\n",
      "Epoch 19 :\n",
      "---------------- \n",
      "train_loss=59.278 (conf=34.247, class=0.919, coords=24.112), val_loss=29.644 (conf=3.416, class=0.956, coords=25.271)\n",
      "\n",
      "Epoch 20 :\n",
      "---------------- \n",
      "train_loss=241.679 (conf=216.339, class=0.907, coords=24.432), val_loss=29.601 (conf=3.372, class=0.957, coords=25.272)\n",
      "\n",
      "Epoch 21 :\n",
      "---------------- \n",
      "train_loss=3312.189 (conf=3284.086, class=0.904, coords=27.200), val_loss=29.520 (conf=3.319, class=0.957, coords=25.244)\n",
      "\n",
      "Epoch 22 :\n",
      "---------------- \n",
      "train_loss=80.967 (conf=49.475, class=0.903, coords=30.589), val_loss=29.476 (conf=3.288, class=0.958, coords=25.230)\n",
      "\n",
      "Epoch 23 :\n",
      "---------------- \n",
      "train_loss=371.224 (conf=339.310, class=0.893, coords=31.021), val_loss=29.442 (conf=3.274, class=0.959, coords=25.208)\n",
      "\n",
      "Epoch 24 :\n",
      "---------------- \n",
      "train_loss=163515.938 (conf=163486.828, class=0.887, coords=28.200), val_loss=29.982 (conf=3.754, class=0.961, coords=25.268)\n",
      "\n",
      "Epoch 25 :\n",
      "---------------- \n",
      "train_loss=7821.142 (conf=7791.130, class=0.890, coords=29.124), val_loss=30.135 (conf=3.936, class=0.962, coords=25.238)\n",
      "\n",
      "Epoch 26 :\n",
      "---------------- \n",
      "train_loss=164.084 (conf=133.263, class=0.894, coords=29.927), val_loss=30.142 (conf=3.965, class=0.962, coords=25.215)\n",
      "\n",
      "Epoch 27 :\n",
      "---------------- \n",
      "train_loss=575.344 (conf=546.317, class=0.883, coords=28.144), val_loss=30.181 (conf=4.000, class=0.962, coords=25.219)\n",
      "\n",
      "Epoch 28 :\n",
      "---------------- \n",
      "train_loss=65.025 (conf=35.344, class=0.882, coords=28.798), val_loss=30.226 (conf=4.022, class=0.964, coords=25.241)\n",
      "\n",
      "Epoch 29 :\n",
      "---------------- \n",
      "train_loss=210.548 (conf=182.627, class=0.886, coords=27.034), val_loss=30.161 (conf=3.983, class=0.964, coords=25.214)\n",
      "\n",
      "Epoch 30 :\n",
      "---------------- \n",
      "train_loss=61.867 (conf=35.094, class=0.885, coords=25.887), val_loss=30.165 (conf=4.022, class=0.963, coords=25.181)\n",
      "\n",
      "Epoch 31 :\n",
      "---------------- \n",
      "train_loss=109.069 (conf=80.092, class=0.874, coords=28.104), val_loss=30.083 (conf=3.994, class=0.962, coords=25.128)\n",
      "\n",
      "Epoch 32 :\n",
      "---------------- \n",
      "train_loss=606.535 (conf=577.256, class=0.863, coords=28.416), val_loss=30.057 (conf=3.949, class=0.964, coords=25.144)\n",
      "\n",
      "Epoch 33 :\n",
      "---------------- \n",
      "train_loss=445.063 (conf=417.717, class=0.885, coords=26.461), val_loss=29.975 (conf=3.952, class=0.960, coords=25.063)\n",
      "\n",
      "Epoch 34 :\n",
      "---------------- \n",
      "train_loss=80.469 (conf=53.029, class=0.872, coords=26.567), val_loss=29.996 (conf=3.964, class=0.961, coords=25.072)\n",
      "\n",
      "Epoch 35 :\n",
      "---------------- \n",
      "train_loss=68.602 (conf=39.903, class=0.868, coords=27.830), val_loss=30.007 (conf=3.972, class=0.964, coords=25.072)\n",
      "\n",
      "Epoch 36 :\n",
      "---------------- \n",
      "train_loss=50.431 (conf=19.195, class=0.874, coords=30.362), val_loss=29.901 (conf=3.939, class=0.959, coords=25.003)\n",
      "\n",
      "Epoch 37 :\n",
      "---------------- \n",
      "train_loss=2263.702 (conf=2238.479, class=0.860, coords=24.363), val_loss=29.924 (conf=3.952, class=0.959, coords=25.013)\n",
      "\n",
      "Epoch 38 :\n",
      "---------------- \n",
      "train_loss=504.574 (conf=475.846, class=0.867, coords=27.861), val_loss=29.924 (conf=3.969, class=0.958, coords=24.997)\n",
      "\n",
      "Epoch 39 :\n",
      "---------------- \n",
      "train_loss=26168.938 (conf=26140.576, class=0.846, coords=27.511), val_loss=30.147 (conf=4.153, class=0.959, coords=25.035)\n",
      "\n",
      "Epoch 40 :\n",
      "---------------- \n",
      "train_loss=97.116 (conf=70.419, class=0.861, coords=25.836), val_loss=30.397 (conf=4.414, class=0.958, coords=25.025)\n",
      "\n",
      "Epoch 41 :\n",
      "---------------- \n",
      "train_loss=72.053 (conf=43.142, class=0.855, coords=28.056), val_loss=30.333 (conf=4.355, class=0.958, coords=25.021)\n",
      "\n",
      "Epoch 42 :\n",
      "---------------- \n",
      "train_loss=586.458 (conf=559.140, class=0.869, coords=26.449), val_loss=30.320 (conf=4.356, class=0.957, coords=25.008)\n",
      "\n",
      "Epoch 43 :\n",
      "---------------- \n",
      "train_loss=106.436 (conf=78.277, class=0.862, coords=27.297), val_loss=30.401 (conf=4.411, class=0.959, coords=25.030)\n",
      "\n",
      "Epoch 44 :\n",
      "---------------- \n",
      "train_loss=74.346 (conf=46.782, class=0.855, coords=26.710), val_loss=30.511 (conf=4.488, class=0.962, coords=25.061)\n",
      "\n",
      "Epoch 45 :\n",
      "---------------- \n",
      "train_loss=49.341 (conf=18.851, class=0.853, coords=29.637), val_loss=30.562 (conf=4.535, class=0.962, coords=25.065)\n",
      "\n",
      "Epoch 46 :\n",
      "---------------- \n",
      "train_loss=152.888 (conf=123.641, class=0.854, coords=28.394), val_loss=30.583 (conf=4.568, class=0.962, coords=25.053)\n",
      "\n",
      "Epoch 47 :\n",
      "---------------- \n",
      "train_loss=223.694 (conf=193.432, class=0.853, coords=29.408), val_loss=30.354 (conf=4.439, class=0.960, coords=24.955)\n",
      "\n",
      "Epoch 48 :\n",
      "---------------- \n",
      "train_loss=161.201 (conf=132.509, class=0.838, coords=27.854), val_loss=30.344 (conf=4.464, class=0.959, coords=24.921)\n",
      "\n",
      "Epoch 49 :\n",
      "---------------- \n",
      "train_loss=185.585 (conf=157.912, class=0.833, coords=26.840), val_loss=30.383 (conf=4.508, class=0.961, coords=24.914)\n",
      "\n",
      "Epoch 50 :\n",
      "---------------- \n",
      "train_loss=182.164 (conf=151.039, class=0.840, coords=30.285), val_loss=30.306 (conf=4.456, class=0.960, coords=24.891)\n",
      "\n",
      "Epoch 51 :\n",
      "---------------- \n",
      "train_loss=940.501 (conf=913.325, class=0.844, coords=26.332), val_loss=30.221 (conf=4.404, class=0.959, coords=24.857)\n",
      "\n",
      "Epoch 52 :\n",
      "---------------- \n",
      "train_loss=232.155 (conf=202.511, class=0.843, coords=28.801), val_loss=30.243 (conf=4.438, class=0.962, coords=24.843)\n",
      "\n",
      "Epoch 53 :\n",
      "---------------- \n",
      "train_loss=100.493 (conf=66.856, class=0.821, coords=32.816), val_loss=30.328 (conf=4.473, class=0.965, coords=24.890)\n",
      "\n",
      "Epoch 54 :\n",
      "---------------- \n",
      "train_loss=180.898 (conf=154.265, class=0.837, coords=25.796), val_loss=30.307 (conf=4.480, class=0.964, coords=24.863)\n",
      "\n",
      "Epoch 55 :\n",
      "---------------- \n",
      "train_loss=151.355 (conf=120.219, class=0.838, coords=30.298), val_loss=30.142 (conf=4.379, class=0.964, coords=24.799)\n",
      "\n",
      "Epoch 56 :\n",
      "---------------- \n",
      "train_loss=2126.075 (conf=2099.659, class=0.844, coords=25.571), val_loss=30.148 (conf=4.400, class=0.963, coords=24.785)\n",
      "\n",
      "Epoch 57 :\n",
      "---------------- \n",
      "train_loss=360.185 (conf=330.648, class=0.829, coords=28.708), val_loss=30.107 (conf=4.357, class=0.963, coords=24.787)\n",
      "\n",
      "Epoch 58 :\n",
      "---------------- \n",
      "train_loss=512.048 (conf=485.298, class=0.841, coords=25.908), val_loss=30.025 (conf=4.310, class=0.961, coords=24.755)\n",
      "\n",
      "Epoch 59 :\n",
      "---------------- \n",
      "train_loss=60.557 (conf=32.210, class=0.828, coords=27.519), val_loss=29.985 (conf=4.274, class=0.961, coords=24.750)\n",
      "\n",
      "Epoch 60 :\n",
      "---------------- \n",
      "train_loss=634.639 (conf=604.580, class=0.831, coords=29.228), val_loss=30.111 (conf=4.368, class=0.962, coords=24.781)\n",
      "\n",
      "Epoch 61 :\n",
      "---------------- \n",
      "train_loss=192.556 (conf=163.771, class=0.836, coords=27.949), val_loss=30.104 (conf=4.347, class=0.962, coords=24.795)\n",
      "\n",
      "Epoch 62 :\n",
      "---------------- \n",
      "train_loss=44.065 (conf=17.073, class=0.825, coords=26.166), val_loss=30.116 (conf=4.393, class=0.963, coords=24.761)\n",
      "\n",
      "Epoch 63 :\n",
      "---------------- \n",
      "train_loss=186.977 (conf=159.738, class=0.801, coords=26.438), val_loss=30.124 (conf=4.439, class=0.961, coords=24.724)\n",
      "\n",
      "Epoch 64 :\n",
      "---------------- \n",
      "train_loss=495.391 (conf=470.599, class=0.827, coords=23.965), val_loss=30.067 (conf=4.436, class=0.960, coords=24.671)\n",
      "\n",
      "Epoch 65 :\n",
      "---------------- \n",
      "train_loss=149.858 (conf=120.265, class=0.822, coords=28.771), val_loss=30.056 (conf=4.407, class=0.961, coords=24.688)\n",
      "\n",
      "Epoch 66 :\n",
      "---------------- \n",
      "train_loss=135.954 (conf=110.457, class=0.809, coords=24.687), val_loss=30.048 (conf=4.399, class=0.960, coords=24.689)\n",
      "\n",
      "Epoch 67 :\n",
      "----------------\n",
      "find better model for train\n",
      " \n",
      "train_loss=38.629 (conf=11.808, class=0.811, coords=26.009), val_loss=30.037 (conf=4.420, class=0.959, coords=24.658)\n",
      "\n",
      "Epoch 68 :\n",
      "---------------- \n",
      "train_loss=347.855 (conf=319.289, class=0.812, coords=27.754), val_loss=30.058 (conf=4.436, class=0.960, coords=24.661)\n",
      "\n",
      "Epoch 69 :\n",
      "---------------- \n",
      "train_loss=265.065 (conf=236.372, class=0.808, coords=27.885), val_loss=30.071 (conf=4.450, class=0.959, coords=24.662)\n",
      "\n",
      "Epoch 70 :\n",
      "---------------- \n",
      "train_loss=6365.783 (conf=6336.746, class=0.817, coords=28.221), val_loss=29.974 (conf=4.432, class=0.957, coords=24.585)\n",
      "\n",
      "Epoch 71 :\n",
      "---------------- \n",
      "train_loss=733.615 (conf=704.074, class=0.819, coords=28.722), val_loss=29.764 (conf=4.320, class=0.956, coords=24.487)\n",
      "\n",
      "Epoch 72 :\n",
      "---------------- \n",
      "train_loss=1215.169 (conf=1188.444, class=0.815, coords=25.910), val_loss=29.777 (conf=4.381, class=0.956, coords=24.440)\n",
      "\n",
      "Epoch 73 :\n",
      "---------------- \n",
      "train_loss=552.996 (conf=522.075, class=0.813, coords=30.108), val_loss=29.914 (conf=4.474, class=0.960, coords=24.480)\n",
      "\n",
      "Epoch 74 :\n",
      "---------------- \n",
      "train_loss=76.338 (conf=49.931, class=0.810, coords=25.596), val_loss=29.846 (conf=4.420, class=0.959, coords=24.467)\n",
      "\n",
      "Epoch 75 :\n",
      "---------------- \n",
      "train_loss=380.621 (conf=353.949, class=0.810, coords=25.862), val_loss=29.788 (conf=4.386, class=0.957, coords=24.444)\n",
      "\n",
      "Epoch 76 :\n",
      "---------------- \n",
      "train_loss=159.320 (conf=132.952, class=0.830, coords=25.537), val_loss=29.773 (conf=4.427, class=0.955, coords=24.392)\n",
      "\n",
      "Epoch 77 :\n",
      "---------------- \n",
      "train_loss=86.257 (conf=57.275, class=0.790, coords=28.192), val_loss=29.710 (conf=4.394, class=0.954, coords=24.362)\n",
      "\n",
      "Epoch 78 :\n",
      "---------------- \n",
      "train_loss=57.645 (conf=28.929, class=0.779, coords=27.937), val_loss=29.775 (conf=4.399, class=0.956, coords=24.420)\n",
      "\n",
      "Epoch 79 :\n",
      "---------------- \n",
      "train_loss=39.940 (conf=13.311, class=0.820, coords=25.809), val_loss=29.744 (conf=4.414, class=0.954, coords=24.376)\n",
      "\n",
      "Epoch 80 :\n",
      "---------------- \n",
      "train_loss=92.103 (conf=63.569, class=0.800, coords=27.734), val_loss=29.784 (conf=4.457, class=0.955, coords=24.373)\n",
      "\n",
      "Epoch 81 :\n",
      "---------------- \n",
      "train_loss=41.906 (conf=14.950, class=0.805, coords=26.150), val_loss=29.770 (conf=4.439, class=0.955, coords=24.376)\n",
      "\n",
      "Epoch 82 :\n",
      "---------------- \n",
      "train_loss=44.057 (conf=14.930, class=0.819, coords=28.307), val_loss=29.735 (conf=4.434, class=0.952, coords=24.349)\n",
      "\n",
      "Epoch 83 :\n",
      "---------------- \n",
      "train_loss=64.421 (conf=38.711, class=0.812, coords=24.898), val_loss=29.688 (conf=4.435, class=0.952, coords=24.301)\n",
      "\n",
      "Epoch 84 :\n",
      "---------------- \n",
      "train_loss=236.581 (conf=209.855, class=0.793, coords=25.933), val_loss=29.635 (conf=4.375, class=0.952, coords=24.308)\n",
      "\n",
      "Epoch 85 :\n",
      "---------------- \n",
      "train_loss=47.781 (conf=19.486, class=0.804, coords=27.491), val_loss=29.646 (conf=4.392, class=0.952, coords=24.302)\n",
      "\n",
      "Epoch 86 :\n",
      "---------------- \n",
      "train_loss=335.622 (conf=306.441, class=0.784, coords=28.397), val_loss=29.667 (conf=4.439, class=0.952, coords=24.275)\n",
      "\n",
      "Epoch 87 :\n",
      "---------------- \n",
      "train_loss=276.889 (conf=251.219, class=0.804, coords=24.866), val_loss=29.701 (conf=4.531, class=0.952, coords=24.218)\n",
      "\n",
      "Epoch 88 :\n",
      "---------------- \n",
      "train_loss=102.853 (conf=73.511, class=0.780, coords=28.562), val_loss=29.632 (conf=4.472, class=0.952, coords=24.207)\n",
      "\n",
      "Epoch 89 :\n",
      "---------------- \n",
      "train_loss=90.303 (conf=66.271, class=0.817, coords=23.214), val_loss=29.522 (conf=4.395, class=0.951, coords=24.176)\n",
      "\n",
      "Epoch 90 :\n",
      "---------------- \n",
      "train_loss=637.089 (conf=609.525, class=0.788, coords=26.776), val_loss=29.438 (conf=4.345, class=0.948, coords=24.145)\n",
      "\n",
      "Epoch 91 :\n",
      "---------------- \n",
      "train_loss=69.942 (conf=43.216, class=0.788, coords=25.938), val_loss=29.490 (conf=4.437, class=0.946, coords=24.107)\n",
      "\n",
      "Epoch 92 :\n",
      "---------------- \n",
      "train_loss=44.869 (conf=17.702, class=0.774, coords=26.394), val_loss=29.491 (conf=4.443, class=0.945, coords=24.103)\n",
      "\n",
      "Epoch 93 :\n",
      "---------------- \n",
      "train_loss=102.668 (conf=74.577, class=0.791, coords=27.301), val_loss=29.503 (conf=4.462, class=0.944, coords=24.096)\n",
      "\n",
      "Epoch 94 :\n",
      "---------------- \n",
      "train_loss=859.185 (conf=831.571, class=0.766, coords=26.848), val_loss=29.450 (conf=4.451, class=0.942, coords=24.057)\n",
      "\n",
      "Epoch 95 :\n",
      "---------------- \n",
      "train_loss=210.781 (conf=181.205, class=0.771, coords=28.805), val_loss=29.507 (conf=4.507, class=0.946, coords=24.055)\n",
      "\n",
      "Epoch 96 :\n",
      "----------------\n",
      "find better model for train\n",
      " \n",
      "train_loss=35.707 (conf=9.168, class=0.784, coords=25.755), val_loss=29.453 (conf=4.510, class=0.944, coords=23.999)\n",
      "\n",
      "Epoch 97 :\n",
      "---------------- \n",
      "train_loss=7508.066 (conf=7481.514, class=0.761, coords=25.791), val_loss=29.493 (conf=4.544, class=0.943, coords=24.005)\n",
      "\n",
      "Epoch 98 :\n",
      "---------------- \n",
      "train_loss=45.473 (conf=21.357, class=0.772, coords=23.344), val_loss=29.680 (conf=4.711, class=0.944, coords=24.025)\n",
      "\n",
      "Epoch 99 :\n",
      "---------------- \n",
      "train_loss=75.841 (conf=49.030, class=0.763, coords=26.049), val_loss=29.588 (conf=4.629, class=0.941, coords=24.018)\n",
      "\n",
      "Epoch 100 :\n",
      "---------------- \n",
      "train_loss=50.311 (conf=24.566, class=0.764, coords=24.981), val_loss=29.641 (conf=4.662, class=0.942, coords=24.037)\n",
      "\n",
      "Epoch 101 :\n",
      "---------------- \n",
      "train_loss=55.988 (conf=27.544, class=0.788, coords=27.655), val_loss=29.430 (conf=4.517, class=0.940, coords=23.973)\n",
      "\n",
      "Epoch 102 :\n",
      "---------------- \n",
      "train_loss=653.020 (conf=623.933, class=0.773, coords=28.315), val_loss=29.447 (conf=4.552, class=0.941, coords=23.954)\n",
      "\n",
      "Epoch 103 :\n",
      "---------------- \n",
      "train_loss=44.365 (conf=17.876, class=0.785, coords=25.705), val_loss=29.421 (conf=4.458, class=0.942, coords=24.022)\n",
      "\n",
      "Epoch 104 :\n",
      "---------------- \n",
      "train_loss=46.413 (conf=20.645, class=0.763, coords=25.005), val_loss=29.336 (conf=4.416, class=0.943, coords=23.977)\n",
      "\n",
      "Epoch 105 :\n",
      "---------------- \n",
      "train_loss=66.317 (conf=38.279, class=0.760, coords=27.278), val_loss=29.370 (conf=4.460, class=0.943, coords=23.967)\n",
      "\n",
      "Epoch 106 :\n",
      "---------------- \n",
      "train_loss=313.845 (conf=285.205, class=0.780, coords=27.860), val_loss=29.239 (conf=4.365, class=0.941, coords=23.932)\n",
      "\n",
      "Epoch 107 :\n",
      "---------------- \n",
      "train_loss=83.696 (conf=52.590, class=0.751, coords=30.355), val_loss=29.175 (conf=4.312, class=0.941, coords=23.922)\n",
      "\n",
      "Epoch 108 :\n",
      "---------------- \n",
      "train_loss=102.971 (conf=72.569, class=0.759, coords=29.642), val_loss=29.183 (conf=4.329, class=0.943, coords=23.911)\n",
      "\n",
      "Epoch 109 :\n",
      "---------------- \n",
      "train_loss=158.053 (conf=129.108, class=0.756, coords=28.188), val_loss=29.103 (conf=4.295, class=0.940, coords=23.869)\n",
      "\n",
      "Epoch 110 :\n",
      "---------------- \n",
      "train_loss=59.268 (conf=31.477, class=0.748, coords=27.043), val_loss=29.129 (conf=4.308, class=0.940, coords=23.881)\n",
      "\n",
      "Epoch 111 :\n",
      "---------------- \n",
      "train_loss=45.375 (conf=18.385, class=0.756, coords=26.235), val_loss=29.137 (conf=4.343, class=0.940, coords=23.855)\n",
      "\n",
      "Epoch 112 :\n",
      "---------------- \n",
      "train_loss=104.167 (conf=77.044, class=0.762, coords=26.361), val_loss=29.125 (conf=4.355, class=0.940, coords=23.831)\n",
      "\n",
      "Epoch 113 :\n",
      "---------------- \n",
      "train_loss=67.007 (conf=40.668, class=0.746, coords=25.594), val_loss=29.251 (conf=4.482, class=0.940, coords=23.829)\n",
      "\n",
      "Epoch 114 :\n",
      "---------------- \n",
      "train_loss=242.846 (conf=217.020, class=0.745, coords=25.081), val_loss=29.156 (conf=4.423, class=0.940, coords=23.794)\n",
      "\n",
      "Epoch 115 :\n",
      "---------------- \n",
      "train_loss=37.994 (conf=8.148, class=0.744, coords=29.102), val_loss=29.118 (conf=4.416, class=0.939, coords=23.763)\n",
      "\n",
      "Epoch 116 :\n",
      "---------------- \n",
      "train_loss=56.788 (conf=28.673, class=0.758, coords=27.357), val_loss=29.126 (conf=4.471, class=0.935, coords=23.720)\n",
      "\n",
      "Epoch 117 :\n",
      "---------------- \n",
      "train_loss=263.993 (conf=234.987, class=0.745, coords=28.261), val_loss=29.181 (conf=4.504, class=0.935, coords=23.742)\n",
      "\n",
      "Epoch 118 :\n",
      "---------------- \n",
      "train_loss=9437.496 (conf=9413.983, class=0.754, coords=22.760), val_loss=28.409 (conf=3.806, class=0.934, coords=23.669)\n",
      "\n",
      "Epoch 119 :\n",
      "---------------- \n",
      "train_loss=174.615 (conf=144.787, class=0.759, coords=29.069), val_loss=28.149 (conf=3.535, class=0.934, coords=23.680)\n",
      "\n",
      "Epoch 120 :\n",
      "---------------- \n",
      "train_loss=548.763 (conf=521.182, class=0.767, coords=26.813), val_loss=28.110 (conf=3.536, class=0.934, coords=23.640)\n",
      "\n",
      "Epoch 121 :\n",
      "----------------\n",
      "find better model for train\n",
      " \n",
      "train_loss=32.285 (conf=8.798, class=0.735, coords=22.752), val_loss=28.118 (conf=3.570, class=0.932, coords=23.616)\n",
      "\n",
      "Epoch 122 :\n",
      "---------------- \n",
      "train_loss=413.568 (conf=389.016, class=0.749, coords=23.803), val_loss=28.148 (conf=3.596, class=0.932, coords=23.620)\n",
      "\n",
      "Epoch 123 :\n",
      "---------------- \n",
      "train_loss=151.207 (conf=124.186, class=0.738, coords=26.284), val_loss=28.161 (conf=3.634, class=0.932, coords=23.595)\n",
      "\n",
      "Epoch 124 :\n",
      "---------------- \n",
      "train_loss=70.310 (conf=40.077, class=0.724, coords=29.509), val_loss=28.110 (conf=3.602, class=0.932, coords=23.575)\n",
      "\n",
      "Epoch 125 :\n",
      "---------------- \n",
      "train_loss=6570.991 (conf=6544.874, class=0.760, coords=25.357), val_loss=28.110 (conf=3.628, class=0.932, coords=23.550)\n",
      "\n",
      "Epoch 126 :\n",
      "---------------- \n",
      "train_loss=117.771 (conf=90.736, class=0.756, coords=26.280), val_loss=28.122 (conf=3.672, class=0.930, coords=23.520)\n",
      "\n",
      "Epoch 127 :\n",
      "---------------- \n",
      "train_loss=92322.500 (conf=92296.500, class=0.735, coords=25.274), val_loss=27.935 (conf=3.486, class=0.930, coords=23.519)\n",
      "\n",
      "Epoch 128 :\n",
      "---------------- \n",
      "train_loss=111.394 (conf=83.164, class=0.710, coords=27.520), val_loss=27.932 (conf=3.493, class=0.929, coords=23.509)\n",
      "\n",
      "Epoch 129 :\n",
      "---------------- \n",
      "train_loss=7050.997 (conf=7022.036, class=0.751, coords=28.211), val_loss=27.869 (conf=3.450, class=0.930, coords=23.489)\n",
      "\n",
      "Epoch 130 :\n",
      "---------------- \n",
      "train_loss=298.785 (conf=270.013, class=0.744, coords=28.028), val_loss=27.835 (conf=3.434, class=0.929, coords=23.472)\n",
      "\n",
      "Epoch 131 :\n",
      "---------------- \n",
      "train_loss=1355.506 (conf=1331.098, class=0.735, coords=23.673), val_loss=27.824 (conf=3.433, class=0.931, coords=23.460)\n",
      "\n",
      "Epoch 132 :\n",
      "---------------- \n",
      "train_loss=62.656 (conf=35.047, class=0.746, coords=26.863), val_loss=27.833 (conf=3.438, class=0.930, coords=23.465)\n",
      "\n",
      "Epoch 133 :\n",
      "---------------- \n",
      "train_loss=46.704 (conf=21.081, class=0.721, coords=24.902), val_loss=27.837 (conf=3.474, class=0.929, coords=23.434)\n",
      "\n",
      "Epoch 134 :\n",
      "---------------- \n",
      "train_loss=101.255 (conf=74.517, class=0.752, coords=25.986), val_loss=27.781 (conf=3.466, class=0.927, coords=23.389)\n",
      "\n",
      "Epoch 135 :\n",
      "---------------- \n",
      "train_loss=72.420 (conf=46.417, class=0.720, coords=25.283), val_loss=27.724 (conf=3.423, class=0.925, coords=23.375)\n",
      "\n",
      "Epoch 136 :\n",
      "---------------- \n",
      "train_loss=34.691 (conf=7.490, class=0.739, coords=26.463), val_loss=27.742 (conf=3.439, class=0.926, coords=23.377)\n",
      "\n",
      "Epoch 137 :\n",
      "---------------- \n",
      "train_loss=43.575 (conf=16.835, class=0.745, coords=25.995), val_loss=27.725 (conf=3.472, class=0.928, coords=23.325)\n",
      "\n",
      "Epoch 138 :\n",
      "---------------- \n",
      "train_loss=1240.907 (conf=1213.266, class=0.719, coords=26.922), val_loss=27.647 (conf=3.473, class=0.927, coords=23.246)\n",
      "\n",
      "Epoch 139 :\n",
      "---------------- \n",
      "train_loss=347.771 (conf=320.292, class=0.723, coords=26.756), val_loss=27.627 (conf=3.450, class=0.926, coords=23.251)\n",
      "\n",
      "Epoch 140 :\n",
      "---------------- \n",
      "train_loss=314.364 (conf=285.525, class=0.711, coords=28.128), val_loss=27.646 (conf=3.439, class=0.925, coords=23.281)\n",
      "\n",
      "Epoch 141 :\n",
      "---------------- \n",
      "train_loss=107.912 (conf=81.564, class=0.736, coords=25.613), val_loss=27.633 (conf=3.458, class=0.924, coords=23.251)\n",
      "\n",
      "Epoch 142 :\n",
      "---------------- \n",
      "train_loss=79.018 (conf=53.948, class=0.727, coords=24.343), val_loss=27.631 (conf=3.462, class=0.925, coords=23.244)\n",
      "\n",
      "Epoch 143 :\n",
      "---------------- \n",
      "train_loss=100.410 (conf=72.828, class=0.694, coords=26.888), val_loss=27.572 (conf=3.443, class=0.924, coords=23.205)\n",
      "\n",
      "Epoch 144 :\n",
      "---------------- \n",
      "train_loss=88.556 (conf=60.964, class=0.736, coords=26.855), val_loss=27.639 (conf=3.470, class=0.925, coords=23.245)\n",
      "\n",
      "Epoch 145 :\n",
      "---------------- \n",
      "train_loss=44.569 (conf=15.855, class=0.753, coords=27.961), val_loss=27.665 (conf=3.499, class=0.925, coords=23.240)\n",
      "\n",
      "Epoch 146 :\n",
      "---------------- \n",
      "train_loss=50130.977 (conf=50104.773, class=0.713, coords=25.485), val_loss=27.626 (conf=3.496, class=0.924, coords=23.206)\n",
      "\n",
      "Epoch 147 :\n",
      "---------------- \n",
      "train_loss=34931.613 (conf=34904.090, class=0.694, coords=26.831), val_loss=27.914 (conf=3.755, class=0.926, coords=23.233)\n",
      "\n",
      "Epoch 148 :\n",
      "---------------- \n",
      "train_loss=127.697 (conf=98.003, class=0.747, coords=28.948), val_loss=28.062 (conf=3.893, class=0.924, coords=23.244)\n",
      "\n",
      "Epoch 149 :\n",
      "---------------- \n",
      "train_loss=43.509 (conf=18.162, class=0.703, coords=24.644), val_loss=27.955 (conf=3.829, class=0.924, coords=23.202)\n",
      "\n",
      "Epoch 150 :\n",
      "---------------- \n",
      "train_loss=31203242.000 (conf=31203220.000, class=0.702, coords=25.195), val_loss=26.713 (conf=1.944, class=0.925, coords=23.845)\n",
      "\n",
      "Epoch 151 :\n",
      "---------------- \n",
      "train_loss=76.204 (conf=48.929, class=0.728, coords=26.547), val_loss=26.632 (conf=1.912, class=0.925, coords=23.795)\n",
      "\n",
      "Epoch 152 :\n",
      "---------------- \n",
      "train_loss=35.811 (conf=9.676, class=0.710, coords=25.426), val_loss=26.570 (conf=1.881, class=0.924, coords=23.765)\n",
      "\n",
      "Epoch 153 :\n",
      "---------------- \n",
      "train_loss=51.528 (conf=27.861, class=0.728, coords=22.939), val_loss=26.588 (conf=1.933, class=0.922, coords=23.733)\n",
      "\n",
      "Epoch 154 :\n",
      "---------------- \n",
      "train_loss=197854.156 (conf=197827.719, class=0.707, coords=25.720), val_loss=26.552 (conf=1.892, class=0.919, coords=23.741)\n",
      "\n",
      "Epoch 155 :\n",
      "---------------- \n",
      "train_loss=74.991 (conf=48.325, class=0.693, coords=25.973), val_loss=26.532 (conf=1.933, class=0.916, coords=23.683)\n",
      "\n",
      "Epoch 156 :\n",
      "---------------- \n",
      "train_loss=1429.409 (conf=1401.412, class=0.706, coords=27.291), val_loss=26.565 (conf=2.043, class=0.915, coords=23.607)\n",
      "\n",
      "Epoch 157 :\n",
      "---------------- \n",
      "train_loss=42.788 (conf=15.901, class=0.710, coords=26.178), val_loss=26.479 (conf=1.968, class=0.917, coords=23.593)\n",
      "\n",
      "Epoch 158 :\n",
      "---------------- \n",
      "train_loss=92.341 (conf=59.661, class=0.736, coords=31.944), val_loss=26.472 (conf=2.008, class=0.919, coords=23.546)\n",
      "\n",
      "Epoch 159 :\n",
      "---------------- \n",
      "train_loss=677.315 (conf=650.677, class=0.693, coords=25.946), val_loss=26.496 (conf=2.082, class=0.916, coords=23.498)\n",
      "\n",
      "Epoch 160 :\n",
      "---------------- \n",
      "train_loss=139.971 (conf=113.361, class=0.696, coords=25.913), val_loss=26.604 (conf=2.241, class=0.914, coords=23.448)\n",
      "\n",
      "Epoch 161 :\n",
      "---------------- \n",
      "train_loss=350.307 (conf=323.307, class=0.685, coords=26.316), val_loss=26.531 (conf=2.181, class=0.914, coords=23.436)\n",
      "\n",
      "Epoch 162 :\n",
      "---------------- \n",
      "train_loss=372.230 (conf=346.262, class=0.695, coords=25.273), val_loss=26.752 (conf=2.447, class=0.912, coords=23.393)\n",
      "\n",
      "Epoch 163 :\n",
      "---------------- \n",
      "train_loss=75.957 (conf=48.632, class=0.700, coords=26.625), val_loss=26.679 (conf=2.384, class=0.915, coords=23.381)\n",
      "\n",
      "Epoch 164 :\n",
      "---------------- \n",
      "train_loss=105.173 (conf=77.357, class=0.690, coords=27.127), val_loss=26.690 (conf=2.365, class=0.916, coords=23.409)\n",
      "\n",
      "Epoch 165 :\n",
      "---------------- \n",
      "train_loss=67.318 (conf=40.355, class=0.703, coords=26.260), val_loss=26.969 (conf=2.622, class=0.917, coords=23.430)\n",
      "\n",
      "Epoch 166 :\n",
      "---------------- \n",
      "train_loss=76.589 (conf=51.210, class=0.715, coords=24.664), val_loss=26.634 (conf=2.256, class=0.919, coords=23.459)\n",
      "\n",
      "Epoch 167 :\n",
      "---------------- \n",
      "train_loss=444.570 (conf=418.021, class=0.710, coords=25.840), val_loss=26.452 (conf=2.076, class=0.919, coords=23.456)\n",
      "\n",
      "Epoch 168 :\n",
      "---------------- \n",
      "train_loss=98.584 (conf=71.876, class=0.688, coords=26.020), val_loss=26.973 (conf=2.685, class=0.919, coords=23.369)\n",
      "\n",
      "Epoch 169 :\n",
      "---------------- \n",
      "train_loss=46.814 (conf=22.082, class=0.696, coords=24.036), val_loss=27.015 (conf=2.742, class=0.918, coords=23.355)\n",
      "\n",
      "Epoch 170 :\n",
      "---------------- \n",
      "train_loss=39.933 (conf=11.830, class=0.707, coords=27.396), val_loss=27.141 (conf=2.862, class=0.913, coords=23.366)\n",
      "\n",
      "Epoch 171 :\n",
      "---------------- \n",
      "train_loss=57104.398 (conf=57073.961, class=0.706, coords=29.724), val_loss=27.809 (conf=3.545, class=0.914, coords=23.351)\n",
      "\n",
      "Epoch 172 :\n",
      "---------------- \n",
      "train_loss=51.993 (conf=24.694, class=0.689, coords=26.610), val_loss=27.495 (conf=3.227, class=0.914, coords=23.354)\n",
      "\n",
      "Epoch 173 :\n",
      "---------------- \n",
      "train_loss=108.915 (conf=85.101, class=0.688, coords=23.127), val_loss=28.148 (conf=3.934, class=0.910, coords=23.304)\n",
      "\n",
      "Epoch 174 :\n",
      "---------------- \n",
      "train_loss=48.463 (conf=22.726, class=0.708, coords=25.029), val_loss=29.665 (conf=5.465, class=0.909, coords=23.291)\n",
      "\n",
      "Epoch 175 :\n",
      "---------------- \n",
      "train_loss=249.427 (conf=222.003, class=0.670, coords=26.754), val_loss=29.341 (conf=5.135, class=0.908, coords=23.298)\n",
      "\n",
      "Epoch 176 :\n",
      "---------------- \n",
      "train_loss=118.564 (conf=93.487, class=0.676, coords=24.401), val_loss=28.171 (conf=3.954, class=0.908, coords=23.310)\n",
      "\n",
      "Epoch 177 :\n",
      "---------------- \n",
      "train_loss=61.852 (conf=34.148, class=0.694, coords=27.009), val_loss=31.790 (conf=7.626, class=0.905, coords=23.259)\n",
      "\n",
      "Epoch 178 :\n",
      "---------------- \n",
      "train_loss=46.665 (conf=22.919, class=0.654, coords=23.092), val_loss=30.200 (conf=6.025, class=0.904, coords=23.271)\n",
      "\n",
      "Epoch 179 :\n",
      "---------------- \n",
      "train_loss=189.462 (conf=162.873, class=0.688, coords=25.900), val_loss=28.812 (conf=4.628, class=0.905, coords=23.280)\n",
      "\n",
      "Epoch 180 :\n",
      "---------------- \n",
      "train_loss=145.715 (conf=119.572, class=0.708, coords=25.435), val_loss=33.154 (conf=9.056, class=0.903, coords=23.195)\n",
      "\n",
      "Epoch 181 :\n",
      "---------------- \n",
      "train_loss=252.752 (conf=227.353, class=0.734, coords=24.666), val_loss=30.173 (conf=6.044, class=0.902, coords=23.227)\n",
      "\n",
      "Epoch 182 :\n",
      "---------------- \n",
      "train_loss=100.829 (conf=73.419, class=0.649, coords=26.761), val_loss=30.297 (conf=6.180, class=0.899, coords=23.218)\n",
      "\n",
      "Epoch 183 :\n",
      "---------------- \n",
      "train_loss=44.810 (conf=15.860, class=0.656, coords=28.295), val_loss=29.549 (conf=5.469, class=0.900, coords=23.180)\n",
      "\n",
      "Epoch 184 :\n",
      "---------------- \n",
      "train_loss=324.177 (conf=297.378, class=0.684, coords=26.116), val_loss=29.143 (conf=5.072, class=0.898, coords=23.174)\n",
      "\n",
      "Epoch 185 :\n",
      "---------------- \n",
      "train_loss=80.336 (conf=53.053, class=0.667, coords=26.615), val_loss=28.623 (conf=4.575, class=0.896, coords=23.151)\n",
      "\n",
      "Epoch 186 :\n",
      "---------------- \n",
      "train_loss=1250.844 (conf=1226.047, class=0.673, coords=24.123), val_loss=28.367 (conf=4.328, class=0.896, coords=23.143)\n",
      "\n",
      "Epoch 187 :\n",
      "---------------- \n",
      "train_loss=205.549 (conf=179.457, class=0.700, coords=25.391), val_loss=28.344 (conf=4.278, class=0.896, coords=23.171)\n",
      "\n",
      "Epoch 188 :\n",
      "---------------- \n",
      "train_loss=50.861 (conf=21.551, class=0.658, coords=28.652), val_loss=28.243 (conf=4.195, class=0.895, coords=23.153)\n",
      "\n",
      "Epoch 189 :\n",
      "---------------- \n",
      "train_loss=43.801 (conf=14.877, class=0.655, coords=28.268), val_loss=29.645 (conf=5.653, class=0.894, coords=23.098)\n",
      "\n",
      "Epoch 190 :\n",
      "---------------- \n",
      "train_loss=58.447 (conf=32.148, class=0.673, coords=25.626), val_loss=28.379 (conf=4.387, class=0.892, coords=23.099)\n",
      "\n",
      "Epoch 191 :\n",
      "---------------- \n",
      "train_loss=5031.076 (conf=5007.688, class=0.702, coords=22.685), val_loss=28.908 (conf=4.869, class=0.891, coords=23.149)\n",
      "\n",
      "Epoch 192 :\n",
      "---------------- \n",
      "train_loss=52.097 (conf=26.547, class=0.684, coords=24.866), val_loss=28.213 (conf=4.177, class=0.891, coords=23.144)\n",
      "\n",
      "Epoch 193 :\n",
      "---------------- \n",
      "train_loss=2375.826 (conf=2349.103, class=0.662, coords=26.061), val_loss=35.975 (conf=12.023, class=0.887, coords=23.065)\n",
      "\n",
      "Epoch 194 :\n",
      "---------------- \n",
      "train_loss=93579.664 (conf=93554.734, class=0.669, coords=24.276), val_loss=34.111 (conf=10.195, class=0.887, coords=23.029)\n",
      "\n",
      "Epoch 195 :\n",
      "---------------- \n",
      "train_loss=213.610 (conf=185.516, class=0.670, coords=27.424), val_loss=29.102 (conf=5.160, class=0.887, coords=23.055)\n",
      "\n",
      "Epoch 196 :\n",
      "---------------- \n",
      "train_loss=164.639 (conf=136.471, class=0.664, coords=27.504), val_loss=27.563 (conf=3.603, class=0.888, coords=23.072)\n",
      "\n",
      "Epoch 197 :\n",
      "---------------- \n",
      "train_loss=5911.006 (conf=5886.121, class=0.683, coords=24.202), val_loss=27.970 (conf=4.027, class=0.889, coords=23.054)\n",
      "\n",
      "Epoch 198 :\n",
      "---------------- \n",
      "train_loss=91.551 (conf=64.474, class=0.665, coords=26.412), val_loss=28.040 (conf=4.092, class=0.888, coords=23.060)\n",
      "\n",
      "Epoch 199 :\n",
      "---------------- \n",
      "train_loss=155.156 (conf=130.598, class=0.641, coords=23.917), val_loss=27.670 (conf=3.689, class=0.888, coords=23.094)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'weight/training_1_epoch_final_666.h5'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    epoch_sub_loss = []\n",
    "    epoch_val_loss = []\n",
    "    epoch_val_sub_loss = []\n",
    "    print('\\nEpoch {} :'.format(epoch))\n",
    "\n",
    "    # train\n",
    "    for bs_idx, (x, y) in enumerate(dataset_train):\n",
    "        x, detector_mask, y_true_anchor_boxes, y_true_class_hot, y_true_boxes_all = transform(x, y, cfg)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x, training=True)\n",
    "            loss, sub_loss = yolo_loss(detector_mask, y_true_anchor_boxes, y_true_class_hot, y_true_boxes_all, y_pred, cfg)\n",
    "            _loss = loss * 0.01\n",
    "        grads = tape.gradient(_loss, train_vars)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, train_vars))\n",
    "        epoch_loss.append(loss)\n",
    "        epoch_sub_loss.append(sub_loss)\n",
    "        if (bs_idx + 1) % (math.ceil(num_iterations / n_progress)) == 0:\n",
    "            print('-', end='')\n",
    "        if (bs_idx + 1) == num_iterations:\n",
    "            break\n",
    "\n",
    "    # val\n",
    "    for bs_idx, (x,y) in enumerate(dataset_val):\n",
    "        x, detector_mask, y_true_anchor_boxes, y_true_class_hot, y_true_boxes_all = transform(x, y, cfg)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x, training=False)\n",
    "            loss, sub_loss = yolo_loss(detector_mask, y_true_anchor_boxes, y_true_class_hot, y_true_boxes_all, y_pred, cfg)\n",
    "        epoch_val_loss.append(loss)\n",
    "        epoch_val_sub_loss.append(sub_loss)\n",
    "        print('-', end='')\n",
    "        if (bs_idx+1)==1:\n",
    "            break\n",
    "\n",
    "    # 记录\n",
    "    loss_avg = np.mean(np.array(epoch_loss))\n",
    "    sub_loss_avg = np.mean(np.array(epoch_sub_loss), axis=0)\n",
    "    val_loss_avg = np.mean(np.array(epoch_val_loss))\n",
    "    val_sub_loss_avg = np.mean(np.array(epoch_val_sub_loss), axis=0)\n",
    "\n",
    "    log_loss(loss_avg, val_loss_avg, step=epoch)\n",
    "    train_loss_history.append(loss_avg)\n",
    "    val_loss_history.append(val_loss_avg)\n",
    "\n",
    "    if loss_avg < best_val_loss:\n",
    "        print('\\nfind better model for train')\n",
    "        best_model_path = save_best_weights(model, train_name+'_epoch%d' % epoch, loss_avg)\n",
    "        best_val_loss = loss_avg\n",
    "    print(' \\ntrain_loss={:.3f} (conf={:.3f}, class={:.3f}, coords={:.3f}), val_loss={:.3f} (conf={:.3f}, class={:.3f}, coords={:.3f})'.format(\n",
    "            loss_avg, sub_loss_avg[0], sub_loss_avg[1], sub_loss_avg[2], val_loss_avg, val_sub_loss_avg[0], val_sub_loss_avg[1], val_sub_loss_avg[2]))\n",
    "save_best_weights(model, train_name+'_epoch_final', 666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.840293,
     "end_time": "2021-01-14T07:01:13.859189",
     "exception": false,
     "start_time": "2021-01-14T07:01:13.018896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T07:01:15.681775Z",
     "iopub.status.busy": "2021-01-14T07:01:15.635531Z",
     "iopub.status.idle": "2021-01-14T07:01:17.254968Z",
     "shell.execute_reply": "2021-01-14T07:01:17.255473Z"
    },
    "papermill": {
     "duration": 2.544634,
     "end_time": "2021-01-14T07:01:17.255607",
     "exception": false,
     "start_time": "2021-01-14T07:01:14.710973",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_img(img, boxes, labels, scores, config):\n",
    "    \"\"\"\n",
    "    画图\n",
    "    :param img: 图片\n",
    "    :param boxes: 物体位置box\n",
    "    :param labels: 标签\n",
    "    :param scores: 得分\n",
    "    :param config: 配置\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 画图\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    img = img.numpy()\n",
    "    boxes = boxes.numpy().reshape(-1, 4)\n",
    "    labels = labels.numpy()\n",
    "    scores = scores.numpy()\n",
    "\n",
    "    ax.show(img)\n",
    "    img_h, img_w = img.shape[0: 2]\n",
    "    colors = ['r', 'orange', 'g', 'b', 'pink', 'purple']\n",
    "    count = 0\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        cx, cy, w, h = box\n",
    "        if w * h <= 0:\n",
    "            continue\n",
    "        count += 1\n",
    "        cx = cx / config.GRID_W * img_w\n",
    "        cy = cy/config.GRID_H * img_h\n",
    "        w = w/config.GRID_W * img_w\n",
    "        h = h/config.GRID_H * img_h\n",
    "        name = cfg.VOC_LABEL_NAME[label+1]\n",
    "        ax.scatter(cx, cy, s=10, c='yellow')\n",
    "        text = ' No:%d' % count + '_'+ name + ' %.3f' % score\n",
    "        ax.text(cx-w/2, cy+h/2, text, fontdict={'size':15, 'color':colors[count-1]})\n",
    "        rect = patches.Rectangle((cx-w/2,cy-h/2), w, h, edgecolor=colors[count-1], linewidth=3.0, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "def display_img(input_img, model, score_threshold, iou_threshold, config):\n",
    "    \"\"\"\n",
    "    把标签展示到图片上\n",
    "    :param input_img: 测试图片\n",
    "    :param model: yolo模型\n",
    "    :param score_threshold: 根据box_conf * box_class_prob筛选boxes\n",
    "    :param iou_threshold: 对boxes做NMS时的阈值\n",
    "    :param config: 配置\n",
    "    :return: box位置\n",
    "    \"\"\"\n",
    "    y_pred = model.predict_on_batch(tf.expand_dims(input_img, 0))\n",
    "    cell_coord_x = tf.cast(tf.reshape(tf.tile(tf.range(config.GRID_W), [config.GRID_H]), (1, config.GRID_H, config.GRID_W, 1, 1)), tf.float32)\n",
    "    cell_coord_y = tf.transpose(cell_coord_x, (0,2,1,3,4))\n",
    "    cell_coords = tf.tile(tf.concat([cell_coord_x, cell_coord_y], -1), [y_pred.shape[0], 1, 1, 5, 1])\n",
    "    anchors = config.ANCHORS\n",
    "\n",
    "    pred_xy = K.sigmoid(y_pred[:,:,:,:,0:2])\n",
    "    pred_xy = pred_xy + cell_coords\n",
    "    pred_wh = K.exp(y_pred[:,:,:,:,2:4]) * anchors\n",
    "\n",
    "    box_conf = K.sigmoid(y_pred[:,:,:,:,4:5])\n",
    "    box_class_prob = K.softmax(y_pred[:,:,:,:,5:])\n",
    "    box_xy1 = pred_xy - 0.5 * pred_wh\n",
    "    box_xy2 = pred_xy + 0.5 * pred_wh\n",
    "    boxes = K.concatenate((box_xy1, box_xy2), axis=-1)\n",
    "\n",
    "    box_scores = box_conf * box_class_prob\n",
    "\n",
    "    box_classes = K.argmax(box_scores, axis=-1) # 最好的分数的 index\n",
    "    box_class_scores = K.max(box_scores, axis=-1) # 最好的分数\n",
    "    prediction_mask = box_class_scores >= score_threshold\n",
    "    boxes = tf.boolean_mask(boxes, prediction_mask)\n",
    "    scores = tf.boolean_mask(box_class_scores, prediction_mask)\n",
    "    classes = tf.boolean_mask(box_classes, prediction_mask)\n",
    "\n",
    "    # NMS（非极大值抑制）\n",
    "    selected_idx = tf.image.non_max_suppression(boxes, scores, 50, iou_threshold=iou_threshold)\n",
    "    boxes = K.gather(boxes, selected_idx)  # [n, 4]\n",
    "    scores = K.gather(scores, selected_idx)  # [n,]\n",
    "    classes = K.gather(classes, selected_idx)  # [n,]\n",
    "    _boxes = K.stack((0.5*(boxes[:,0] + boxes[:,2]),\n",
    "                      0.5*(boxes[:,1] + boxes[:,3]),\n",
    "                      boxes[:,2] - boxes[:,0],\n",
    "                      boxes[:,3] - boxes[:,1]),\n",
    "                      axis=-1) # x1, y1, x2, y2 ==> cx, cy, w, h\n",
    "\n",
    "    return _boxes\n",
    "\n",
    "model.load_weights(best_model_path)\n",
    "for bs_idx, (x,y) in enumerate(dataset_val):\n",
    "    x, detector_mask, y_true_anchor_boxes, y_true_class_hot, y_true_boxes_all = transform(x, y, cfg)\n",
    "    score_threshold = 0.5\n",
    "    iou_threshold = 0.45\n",
    "\n",
    "    display_img(x[0], model, score_threshold, iou_threshold, cfg)\n",
    "    if (bs_idx + 1) == 10:\n",
    "        break\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "ax.plot(train_loss_history[10:])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1320.456612,
   "end_time": "2021-01-14T07:01:19.212838",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-14T06:39:18.756226",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
