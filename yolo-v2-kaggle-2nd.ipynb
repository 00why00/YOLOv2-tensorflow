{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.014364,
     "end_time": "2021-01-15T06:42:38.905333",
     "exception": false,
     "start_time": "2021-01-15T06:42:38.890969",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# YOLO v2\n",
    "## 基于 Tensorflow2.4 的实现\n",
    "## 使用 VOC2007 和 VOC2012 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T06:42:38.938867Z",
     "iopub.status.busy": "2021-01-15T06:42:38.938000Z",
     "iopub.status.idle": "2021-01-15T06:42:40.196407Z",
     "shell.execute_reply": "2021-01-15T06:42:40.195358Z"
    },
    "papermill": {
     "duration": 1.277444,
     "end_time": "2021-01-15T06:42:40.196517",
     "exception": false,
     "start_time": "2021-01-15T06:42:38.919073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir logs\n",
    "!mkdir weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T06:42:40.228879Z",
     "iopub.status.busy": "2021-01-15T06:42:40.228295Z",
     "iopub.status.idle": "2021-01-15T06:42:40.318554Z",
     "shell.execute_reply": "2021-01-15T06:42:40.317988Z"
    },
    "papermill": {
     "duration": 0.108225,
     "end_time": "2021-01-15T06:42:40.318665",
     "exception": false,
     "start_time": "2021-01-15T06:42:40.210440",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导包\n",
    "import os\n",
    "import struct\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T06:42:40.352429Z",
     "iopub.status.busy": "2021-01-15T06:42:40.351630Z",
     "iopub.status.idle": "2021-01-15T06:42:47.703989Z",
     "shell.execute_reply": "2021-01-15T06:42:47.705073Z"
    },
    "papermill": {
     "duration": 7.37222,
     "end_time": "2021-01-15T06:42:47.705244",
     "exception": false,
     "start_time": "2021-01-15T06:42:40.333024",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导包并测试\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014403,
     "end_time": "2021-01-15T06:42:47.749891",
     "exception": false,
     "start_time": "2021-01-15T06:42:47.735488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1、配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T06:42:47.793558Z",
     "iopub.status.busy": "2021-01-15T06:42:47.791199Z",
     "iopub.status.idle": "2021-01-15T06:42:47.796133Z",
     "shell.execute_reply": "2021-01-15T06:42:47.795716Z"
    },
    "papermill": {
     "duration": 0.031896,
     "end_time": "2021-01-15T06:42:47.796219",
     "exception": false,
     "start_time": "2021-01-15T06:42:47.764323",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    参考https://github.com/pjreddie/darknet/blob/master/cfg/yolov2-voc.cfg\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Training\n",
    "        self.IMAGE_W = 416\n",
    "        self.IMAGE_H = 416\n",
    "        self.GRID_W = 13\n",
    "        self.GRID_H = 13\n",
    "        self.ANCHORS_NUM = 5\n",
    "        self.CLASSES_NUM = 20\n",
    "        self.NOOBJECT_LAMBDA = 1\n",
    "        self.OBJECT_LAMBDA = 5\n",
    "        self.CLASS_LAMBDA = 1\n",
    "        self.COORD_LAMBDA = 1\n",
    "        # 论文中提到的K-means算法得到的VOC数据集的5个box的信息\n",
    "        self.ANCHORS = [1.3221, 1.73145, 3.19275, 4.00944, 5.05587, 8.09892, 9.47112, 4.84053, 11.2364, 10.0071]\n",
    "        self.ANCHORS = np.array(self.ANCHORS)\n",
    "        self.ANCHORS = self.ANCHORS.reshape(-1, 2)\n",
    "\n",
    "        self.VOC_NAME_LABEL_CLASS = {\n",
    "            'none': (0, 'Background'),\n",
    "            'aeroplane': (1, 'Vehicle'),\n",
    "            'bicycle': (2, 'Vehicle'),\n",
    "            'bird': (3, 'Animal'),\n",
    "            'boat': (4, 'Vehicle'),\n",
    "            'bottle': (5, 'Indoor'),\n",
    "            'bus': (6, 'Vehicle'),\n",
    "            'car': (7, 'Vehicle'),\n",
    "            'cat': (8, 'Animal'),\n",
    "            'chair': (9, 'Indoor'),\n",
    "            'cow': (10, 'Animal'),\n",
    "            'diningtable': (11, 'Indoor'),\n",
    "            'dog': (12, 'Animal'),\n",
    "            'horse': (13, 'Animal'),\n",
    "            'motorbike': (14, 'Vehicle'),\n",
    "            'person': (15, 'Person'),\n",
    "            'pottedplant': (16, 'Indoor'),\n",
    "            'sheep': (17, 'Animal'),\n",
    "            'sofa': (18, 'Indoor'),\n",
    "            'train': (19, 'Vehicle'),\n",
    "            'tvmonitor': (20, 'Indoor'),\n",
    "        }\n",
    "        self.VOC_NAME_LABEL = {key:v[0] for key,v in self.VOC_NAME_LABEL_CLASS.items()}\n",
    "        self.VOC_LABEL_NAME = {v[0]:key for key,v in self.VOC_NAME_LABEL_CLASS.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014401,
     "end_time": "2021-01-15T06:42:47.825306",
     "exception": false,
     "start_time": "2021-01-15T06:42:47.810905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2、处理和构造数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T06:42:47.880535Z",
     "iopub.status.busy": "2021-01-15T06:42:47.867269Z",
     "iopub.status.idle": "2021-01-15T06:42:47.883056Z",
     "shell.execute_reply": "2021-01-15T06:42:47.882583Z"
    },
    "papermill": {
     "duration": 0.043442,
     "end_time": "2021-01-15T06:42:47.883154",
     "exception": false,
     "start_time": "2021-01-15T06:42:47.839712",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_example_list(path):\n",
    "    \"\"\"\n",
    "    读取训练集或验证集\n",
    "    :param path: 路径\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    with tf.io.gfile.GFile(path) as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip().split(' ')[0] for line in lines]\n",
    "\n",
    "def parse_xml_to_dict(xml):\n",
    "    \"\"\"\n",
    "    递归地将 xml 转化为 字典\n",
    "    :param xml: 通过解析 xml 得到的 lxml.etree 格式\n",
    "    :return: 包含 xml 的字典\n",
    "    \"\"\"\n",
    "    if len(xml) == 0:\n",
    "        return {xml.tag: xml.text}\n",
    "    result = {}\n",
    "    for child in xml:\n",
    "        child_result = parse_xml_to_dict(child)\n",
    "        if child.tag != 'object':\n",
    "            result[child.tag] = child_result[child.tag]\n",
    "        else:\n",
    "            if child.tag not in result:\n",
    "                result[child.tag] = []\n",
    "            result[child.tag].append(child_result[child.tag])\n",
    "    return {xml.tag: result}\n",
    "\n",
    "def transform(x, y, config):\n",
    "    \"\"\"\n",
    "    从数据集中生成一个 batch size 的标签值， 准备在计算损失时和预测值比较\n",
    "    :param x: 一个 batch size 的图片 (batch size, h, w, 3)\n",
    "    :param y: 一个batch size 的 label (batch size, xmin, ymin, xmax, ymax, label)\n",
    "    :param config: 配置\n",
    "    :return: batch\n",
    "        - x : 要预测的图片（batch_size, IMAGE_H, IMAGE_W, 3）\n",
    "        - detector_mask : 是否有 bounding box 在格子内预测（batch, size, GRID_W, GRID_H, anchors_num, 1）\n",
    "        - y_true_anchor_boxes : bounding box 坐标（batch_size, GRID_W, GRID_H, anchors_num, 5）\n",
    "        - y_true_class_hot : 预测类别的 one hot 编码（batch_size, GRID_W, GRID_H, anchors_num, class_num)\n",
    "        - y_true_boxes_all : 标签值（batch_size, max annotation(这里设置为100), 5）\n",
    "    \"\"\"\n",
    "    anchors = config.ANCHORS\n",
    "    anchors_num = anchors.shape[0]\n",
    "    y = y.numpy()\n",
    "    batch_size = y.shape[0]\n",
    "    detector_mask = np.zeros([batch_size, config.GRID_W, config.GRID_H, anchors_num, 1])\n",
    "    y_true_anchor_boxes = np.zeros([batch_size, config.GRID_W, config.GRID_H, anchors_num, 5])\n",
    "    y_true_class_hot = np.zeros([batch_size, config.GRID_W, config.GRID_H, anchors_num, config.CLASSES_NUM])\n",
    "    y_true_boxes_all = np.zeros(y.shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        boxes = y[i]\n",
    "        for j, box in enumerate(boxes):\n",
    "            w = box[2] - box[0]\n",
    "            h = box[3] - box[1]\n",
    "            cx = (box[0] + box[2])/2  # 中心点坐标\n",
    "            cy = (box[1] + box[3])/2\n",
    "\n",
    "            w *= config.GRID_W\n",
    "            h *= config.GRID_H\n",
    "            cx *= config.GRID_W\n",
    "            cy *= config.GRID_H\n",
    "\n",
    "            y_true_boxes_all[i, j] = np.array([cx, cy, w, h, box[4]])\n",
    "            if w * h <= 0:\n",
    "                continue\n",
    "            # 网格 index\n",
    "            cell_col = np.floor(cx).astype(np.int)\n",
    "            cell_row = np.floor(cy).astype(np.int)\n",
    "            # 寻找 IoU 最高的 anchor\n",
    "            anchors_w, anchors_h = anchors[:, 0], anchors[:, 1]\n",
    "            intersect = np.minimum(w, anchors_w) * np.minimum(h, anchors_h)\n",
    "            union = anchors_w * anchors_h + w * h - intersect\n",
    "            iou = intersect / union\n",
    "            anchor_best = np.argmax(iou)\n",
    "\n",
    "            class_index = int(box[4])\n",
    "            y_true_anchor_boxes[i, cell_col, cell_row, anchor_best] = [cx, cy, w, h, class_index]\n",
    "            y_true_class_hot[i, cell_col, cell_row, anchor_best, class_index-1] = 1\n",
    "            detector_mask[i, cell_col, cell_row, anchor_best] = 1\n",
    "\n",
    "    detector_mask = tf.convert_to_tensor(detector_mask, dtype='int64')\n",
    "    y_true_anchor_boxes = tf.convert_to_tensor(y_true_anchor_boxes, dtype='float32')\n",
    "    y_true_boxes_all = tf.convert_to_tensor(y_true_boxes_all, dtype='float32')\n",
    "    y_true_class_hot = tf.convert_to_tensor(y_true_class_hot, dtype='float32')\n",
    "    batch = (x, detector_mask, y_true_anchor_boxes, y_true_class_hot, y_true_boxes_all)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T06:42:47.933343Z",
     "iopub.status.busy": "2021-01-15T06:42:47.931397Z",
     "iopub.status.idle": "2021-01-15T06:42:47.933953Z",
     "shell.execute_reply": "2021-01-15T06:42:47.934353Z"
    },
    "papermill": {
     "duration": 0.036984,
     "end_time": "2021-01-15T06:42:47.934493",
     "exception": false,
     "start_time": "2021-01-15T06:42:47.897509",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 法一：先生成tfrecords，读取时再一个个的batch解析\n",
    "IMAGE_FEATURE_MAP = {\n",
    "    'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/key/sha256': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32), # 如果数据中存放的list长度大于1, 表示数据是不定长的, 使用VarLenFeature解析\n",
    "    'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
    "    'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
    "    'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
    "    'image/object/class/text': tf.io.VarLenFeature(tf.string),\n",
    "    'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n",
    "    'image/object/difficult': tf.io.VarLenFeature(tf.int64),\n",
    "    'image/object/truncated': tf.io.VarLenFeature(tf.int64),\n",
    "    'image/object/view': tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "\n",
    "def parse_example(serialized_example, height, width):\n",
    "    x = tf.io.parse_single_example(serialized_example, IMAGE_FEATURE_MAP)\n",
    "    x_train = tf.image.decode_jpeg(x['image/encoded'], channels=3)\n",
    "    x_train = tf.image.resize(x_train, (height,width))\n",
    "    labels = tf.cast(tf.sparse.to_dense(x['image/object/class/label']), tf.float32)\n",
    "    y_train = tf.stack([tf.sparse.to_dense(x['image/object/bbox/xmin']), # shape: [m]\n",
    "                        tf.sparse.to_dense(x['image/object/bbox/ymin']), # shape: [m]\n",
    "                        tf.sparse.to_dense(x['image/object/bbox/xmax']), # shape: [m]\n",
    "                        tf.sparse.to_dense(x['image/object/bbox/ymax']), # shape: [m]\n",
    "                        labels  # shape: [m]\n",
    "                        ], axis=1) # shape:[m, 5], m是图片中目标的个数, 每张图片的m可能不一样\n",
    "    # 每个图片最多包含100个目标\n",
    "    paddings = [[0, 100 - tf.shape(y_train)[0]], [0, 0]] # 上下左右分别填充0, 100 - tf.shape(y_train)[0], 0, 0\n",
    "    y_train = tf.pad(y_train, paddings)\n",
    "    return x_train, y_train\n",
    "\n",
    "def OB_tfrecord_dataset(dataset_path, batch_size, config, shuffle=False):\n",
    "    files = tf.data.Dataset.list_files(dataset_path)\n",
    "    dataset = files.flat_map(tf.data.TFRecordDataset)\n",
    "    dataset = dataset.map(lambda x:parse_example(x, height=config.IMAGE_H, width=config.IMAGE_W))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=500)\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)  # 提前获取数据存在缓存里来减少gpu因为缺少数据而等待的情况\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T06:42:47.992997Z",
     "iopub.status.busy": "2021-01-15T06:42:47.970528Z",
     "iopub.status.idle": "2021-01-15T06:42:47.995709Z",
     "shell.execute_reply": "2021-01-15T06:42:47.995240Z"
    },
    "papermill": {
     "duration": 0.045125,
     "end_time": "2021-01-15T06:42:47.995793",
     "exception": false,
     "start_time": "2021-01-15T06:42:47.950668",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 法二：即时的从硬盘读取，不用生成tfrecords，不需事先把每张图片的标签读进来\n",
    "def get_imgPath_and_annotations(data_path, years, config,\n",
    "                                image_subdirectory = 'JPEGImages',\n",
    "                                annotations_dir = 'Annotations',\n",
    "                                ignore_difficult_instances = False):\n",
    "    \"\"\"\n",
    "    得到图片的路径和box\n",
    "    :param data_path: 根目录\n",
    "    :param years: 数据集年份 2007/2012\n",
    "    :param config: 配置\n",
    "    :param image_subdirectory: 图片子文件夹\n",
    "    :param annotations_dir: 标签子文件夹\n",
    "    :param ignore_difficult_instances: 忽略复杂数据\n",
    "    :return: 路径 box\n",
    "    \"\"\"\n",
    "    annotations_list = []\n",
    "    for year in years.keys():\n",
    "        sets = years[year]\n",
    "        for _set in sets:\n",
    "            examples_path = os.path.join(data_path, year, 'ImageSets', 'Main', _set + '.txt')\n",
    "            annotations_path = os.path.join(data_path, year, annotations_dir)\n",
    "            examples_list = read_example_list(examples_path)\n",
    "            annotation_list = [os.path.join(annotations_path, example + '.xml') for example in examples_list]\n",
    "            annotations_list += annotation_list\n",
    "\n",
    "    img_names = []\n",
    "    max_obj = 200\n",
    "    annotations = []  # 存放每个图片的box\n",
    "    for path in annotations_list:\n",
    "        with tf.io.gfile.GFile(path, 'r') as f:\n",
    "            xml_str = f.read()\n",
    "        xml = etree.fromstring(xml_str)\n",
    "        data = parse_xml_to_dict(xml)['annotation']\n",
    "        width = int(data['size']['width'])\n",
    "        height = int(data['size']['height'])\n",
    "\n",
    "        boxes = []\n",
    "        if 'object' not in data:\n",
    "            continue\n",
    "        for obj in data['object']:\n",
    "            difficult = bool(int(obj['difficult']))\n",
    "            if ignore_difficult_instances and difficult:\n",
    "                continue\n",
    "            box = np.array([\n",
    "                float(obj['bndbox']['xmin']) / width,\n",
    "                float(obj['bndbox']['ymin']) / height,\n",
    "                float(obj['bndbox']['xmax']) / width,\n",
    "                float(obj['bndbox']['ymax']) / height,\n",
    "                config.VOC_NAME_LABEL[obj['name']]\n",
    "            ])\n",
    "            boxes.append(box)  # 一个图片的box可能有多个\n",
    "        boxes = np.stack(boxes)\n",
    "        annotations.append(boxes)\n",
    "\n",
    "        img_path = os.path.join(data['folder'], image_subdirectory, data['filename'])\n",
    "        img_path = os.path.join(data_path, img_path)\n",
    "        img_names.append(img_path)\n",
    "    true_boxes = np.zeros([len(img_names), max_obj, 5])\n",
    "    for idx, boxes in enumerate(annotations):\n",
    "        true_boxes[idx, :boxes.shape[0]] = boxes\n",
    "    return img_names, true_boxes\n",
    "\n",
    "def parse_image(filename, true_boxes, img_h, img_w):\n",
    "    \"\"\"\n",
    "    得到图片和 box\n",
    "    :param filename: 图片路径\n",
    "    :param true_boxes: box\n",
    "    :param img_h: 期望图片长\n",
    "    :param img_w: 期望图片宽\n",
    "    :return: 图片 box\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, (img_h, img_w))\n",
    "    return image, true_boxes\n",
    "\n",
    "def OB_tensor_slices_dataset(data_path, years, batch_size, config, shuffle=False):\n",
    "    img_names, boxes = get_imgPath_and_annotations(data_path, years, config)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_names, boxes))\n",
    "    dataset = dataset.map(lambda x, y:parse_image(x, y, img_h=config.IMAGE_H, img_w=config.IMAGE_W))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=500)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)  # 提前获取数据存在缓存里来减少gpu因为缺少数据而等待的情况\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014527,
     "end_time": "2021-01-15T06:42:48.025115",
     "exception": false,
     "start_time": "2021-01-15T06:42:48.010588",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### 3、定义 YOLO 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T06:42:48.067334Z",
     "iopub.status.busy": "2021-01-15T06:42:48.056840Z",
     "iopub.status.idle": "2021-01-15T06:42:48.103936Z",
     "shell.execute_reply": "2021-01-15T06:42:48.103465Z"
    },
    "papermill": {
     "duration": 0.064405,
     "end_time": "2021-01-15T06:42:48.104022",
     "exception": false,
     "start_time": "2021-01-15T06:42:48.039617",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def space_to_depth(x):\n",
    "    return tf.nn.space_to_depth(x, block_size=2)\n",
    "\n",
    "def yolo(config):\n",
    "    \"\"\"\n",
    "    训练模型，即去掉最后一个卷积层，转而增加了三个3*3*1024的卷积层,集体参看yolov2-voc.cfg文件\n",
    "    :param config: 配置文件\n",
    "    :return: 网络模型\n",
    "    \"\"\"\n",
    "    input_image = tf.keras.layers.Input((config.IMAGE_H, config.IMAGE_W, 3), dtype='float32')\n",
    "    # 1\n",
    "    x = Conv2D(32, (3, 3), strides=(1, 1), padding='same', name='conv_1', use_bias=False)(input_image)\n",
    "    x = BatchNormalization(name='norm_1')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # 2\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', name='conv_2', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_2')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # 3\n",
    "    x = Conv2D(128, (3, 3), strides=(1, 1), padding='same', name='conv_3', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_3')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 4\n",
    "    x = Conv2D(64, (1, 1), strides=(1, 1), padding='same', name='conv_4', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_4')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 5\n",
    "    x = Conv2D(128, (3, 3), strides=(1, 1), padding='same', name='conv_5', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_5')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # 6\n",
    "    x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_6')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 7\n",
    "    x = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_7')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 8\n",
    "    x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_8')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # 9\n",
    "    x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_9', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_9')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 10\n",
    "    x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_10', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_10')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 11\n",
    "    x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_11')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 12\n",
    "    x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_12', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_12')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 13\n",
    "    x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_13')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    skip_connection = x\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # 14\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_14', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_14')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 15\n",
    "    x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_15', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_15')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 16\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_16', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_16')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 17\n",
    "    x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_17', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_17')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 18\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_18')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 19 把最后一层卷积层移除,添加3个1024*3*3卷积层\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_19', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_19')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 20\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_20', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_20')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 21 passthrough层\n",
    "    skip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\n",
    "    skip_connection = BatchNormalization(name='norm_21')(skip_connection)\n",
    "    skip_connection = LeakyReLU(alpha=0.1)(skip_connection)\n",
    "    skip_connection = Lambda(space_to_depth)(skip_connection)\n",
    "    x = concatenate([skip_connection, x])\n",
    "    # 22\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_22', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_22')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # 23 1*1的卷积层生成预测输出\n",
    "    x = Conv2D(config.ANCHORS_NUM * (4 + 1 + config.CLASSES_NUM), (1,1), strides=(1,1), padding='same', name='conv_23')(x)\n",
    "    output = Reshape((config.GRID_W, config.GRID_H, config.ANCHORS_NUM, 4 + 1 + config.CLASSES_NUM))(x)\n",
    "\n",
    "    yolo_model = tf.keras.models.Model(inputs=input_image, outputs=output)\n",
    "    return yolo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01462,
     "end_time": "2021-01-15T06:42:48.134239",
     "exception": false,
     "start_time": "2021-01-15T06:42:48.119619",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### 4、加载预训练权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T06:42:48.187787Z",
     "iopub.status.busy": "2021-01-15T06:42:48.182947Z",
     "iopub.status.idle": "2021-01-15T06:42:48.189753Z",
     "shell.execute_reply": "2021-01-15T06:42:48.190126Z"
    },
    "papermill": {
     "duration": 0.040283,
     "end_time": "2021-01-15T06:42:48.190230",
     "exception": false,
     "start_time": "2021-01-15T06:42:48.149947",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WeightReader:\n",
    "    def __init__(self, weight_file):\n",
    "        with open(weight_file, 'rb') as w_f:\n",
    "            major, = struct.unpack('i', w_f.read(4))\n",
    "            minor, = struct.unpack('i', w_f.read(4))\n",
    "            revision, = struct.unpack('i', w_f.read(4))\n",
    "            if (major*10+minor) >=2 and major < 1000 and minor < 1000:\n",
    "                w_f.read(8)\n",
    "            else:\n",
    "                w_f.read(4)\n",
    "            transpose = (major > 1000) or (minor > 1000)\n",
    "            binary = w_f.read()\n",
    "        self.offset = 0\n",
    "        self.all_weights = np.frombuffer(binary, dtype='float32')\n",
    "\n",
    "    def read_bytes(self, size):\n",
    "        self.offset = self.offset + size\n",
    "        return self.all_weights[self.offset-size:self.offset]\n",
    "\n",
    "    def load_weights(self, yolo_model, conv_num=23, if_last=False):\n",
    "        conv_num_read = conv_num\n",
    "        if not if_last:\n",
    "            conv_num_read = conv_num - 1\n",
    "\n",
    "        for i in range(1, conv_num_read + 1):\n",
    "            try:\n",
    "                conv_layer = yolo_model.get_layer('conv_' + str(i))\n",
    "                if i < conv_num:\n",
    "                    norm_layer = yolo_model.get_layer('norm_' + str(i))\n",
    "                    size = np.prod(norm_layer.get_weights()[0].shape)\n",
    "                    beta = self.read_bytes(size)  # bias\n",
    "                    gamma = self.read_bytes(size)  # scale\n",
    "                    mean = self.read_bytes(size)  # mean\n",
    "                    var = self.read_bytes(size)  # variance\n",
    "                    weights = norm_layer.set_weights([gamma, beta, mean, var])\n",
    "                if len(conv_layer.get_weights()) > 1:\n",
    "                    bias = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n",
    "                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "                    kernel = kernel.transpose([2, 3, 1, 0])\n",
    "                    conv_layer.set_weights([kernel, bias])\n",
    "                else:\n",
    "                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "                    kernel = kernel.transpose([2,3,1,0])\n",
    "                    conv_layer.set_weights([kernel])\n",
    "            except ValueError:\n",
    "                print(\"no convolution #\" + str(i))\n",
    "\n",
    "        if not if_last:\n",
    "          layer = yolo_model.layers[-2] # last convolutional layer\n",
    "          layer.trainable = True\n",
    "          weights = layer.get_weights()\n",
    "          new_kernel = np.random.normal(size=weights[0].shape)/(13*13)\n",
    "          new_bias = np.random.normal(size=weights[1].shape)/(13*13)\n",
    "          layer.set_weights([new_kernel, new_bias])\n",
    "\n",
    "    def reset(self):\n",
    "        self.offset = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014586,
     "end_time": "2021-01-15T06:42:48.219658",
     "exception": false,
     "start_time": "2021-01-15T06:42:48.205072",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### 5、IoU 和 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T06:42:48.278713Z",
     "iopub.status.busy": "2021-01-15T06:42:48.268188Z",
     "iopub.status.idle": "2021-01-15T06:42:48.294326Z",
     "shell.execute_reply": "2021-01-15T06:42:48.293885Z"
    },
    "papermill": {
     "duration": 0.059541,
     "end_time": "2021-01-15T06:42:48.294426",
     "exception": false,
     "start_time": "2021-01-15T06:42:48.234885",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cal_iou(x1, y1, w1, h1, x2, y2, w2, h2):\n",
    "    \"\"\"\n",
    "    计算 box1 和 box2 的 IoU\n",
    "    \"\"\"\n",
    "    xmin1 = x1 - 0.5 * w1\n",
    "    xmax1 = x1 + 0.5 * w1\n",
    "    ymin1 = y1 - 0.5 * h1\n",
    "    ymax1 = y1 + 0.5 * h1\n",
    "    xmin2 = x2 - 0.5 * w2\n",
    "    xmax2 = x2 + 0.5 * w2\n",
    "    ymin2 = y2 - 0.5 * h2\n",
    "    ymax2 = y2 + 0.5 * h2\n",
    "    inter_x = np.minimum(xmax1, xmax2) - np.maximum(xmin1, xmin2)\n",
    "    inter_y = np.minimum(ymax1, ymax2) - np.maximum(ymin1, ymin2)\n",
    "    inter = inter_x * inter_y\n",
    "    union = w1 * h1 + w2 * h2 - inter\n",
    "    iou = inter / (union + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def yolo_loss(detector_mask, y_true_anchor_boxes, y_true_class_hot, y_true_boxes_all, y_pred, config):\n",
    "    \"\"\"\n",
    "    损失函数\n",
    "    :param detector_mask: shape(batch_size, GRID_W, GRID_H, anchors_count, 1)\n",
    "    :param y_true_anchor_boxes: shape(batch_size, GRID_W, GRID_H, anchors_count, 5)\n",
    "    :param y_true_class_hot: one hot编码 shape(batch_size, GRID_W, GRID_H, anchors_count, class_count)\n",
    "    :param y_true_boxes_all: (x, y, w, h, c)\n",
    "    :param y_pred: shape(batch_size, GRID_W, GRID_H, anchors_count, 5 + labels count)\n",
    "    :param config: 配置\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "    # 0-GRID_W -1 / GRID_H -1\n",
    "    cell_coord_x = tf.cast(tf.reshape(tf.tile(tf.range(config.GRID_W), [config.GRID_H]), (1, config.GRID_H, config.GRID_W, 1, 1)), tf.float32)\n",
    "    cell_coord_y = tf.transpose(cell_coord_x, (0,2,1,3,4))\n",
    "    cell_coords = tf.tile(tf.concat([cell_coord_x, cell_coord_y], -1), [y_pred.shape[0], 1, 1, 5, 1])\n",
    "\n",
    "    # 0-GRID_W / GRID_H\n",
    "    anchors = config.ANCHORS\n",
    "\n",
    "    # 0-GRID_W / GRID_H\n",
    "    pred_xy = K.sigmoid(y_pred[:,:,:,:,0:2])\n",
    "    pred_xy = pred_xy + cell_coords\n",
    "    pred_wh = K.exp(y_pred[:,:,:,:,2:4]) * anchors\n",
    "\n",
    "    # 1、坐标损失（coordinate loss）\n",
    "    # 根据gt的wh计算系数，系数作用是w和h值越小，损失系数越大，可以更好地学习尺度较小的box\n",
    "    lambda_wh = K.expand_dims(2-(y_true_anchor_boxes[:,:,:,:,2]/config.GRID_W) * (y_true_anchor_boxes[:,:,:,:,3]/config.GRID_H))\n",
    "    detector_mask = K.cast(detector_mask, tf.float32)  # batch_size, GRID_W, GRID_H, n_anchors, 1\n",
    "    n_objs = K.sum(K.cast(detector_mask>0, tf.float32))\n",
    "    # 基于预测值计算坐标损失\n",
    "    y_txy = y_true_anchor_boxes[...,0:2] - cell_coords\n",
    "    y_twh = K.log(y_true_anchor_boxes[...,2:4]*1.0/anchors + 1e-16)\n",
    "    pred_txy = K.sigmoid(y_pred[:,:,:,:,0:2])\n",
    "    pred_twh = y_pred[:,:,:,:,2:4]\n",
    "    loss_xy = config.COORD_LAMBDA * K.sum(detector_mask * K.square(y_txy - pred_txy)) / (n_objs + 1e-6)\n",
    "    loss_wh = config.COORD_LAMBDA * K.sum(lambda_wh * detector_mask * K.square(y_twh - pred_twh)) / (n_objs + 1e-6)\n",
    "    loss_coord = loss_xy + loss_wh\n",
    "\n",
    "    # 2、类别损失\n",
    "    pred_class = K.softmax(y_pred[:,:,:,:,5:])\n",
    "    loss_cls = detector_mask * K.square(y_true_class_hot - pred_class)\n",
    "    loss_cls = config.CLASS_LAMBDA * K.sum(loss_cls) / (n_objs + 1e-6)\n",
    "\n",
    "    # 3、bounding box置信度损失\n",
    "    # 3.1、包含目标的预测的bounding box置信度损失\n",
    "    # 预测值和标记值的 IoU\n",
    "    x1 = y_true_anchor_boxes[...,0]\n",
    "    y1 = y_true_anchor_boxes[...,1]\n",
    "    w1 = y_true_anchor_boxes[...,2]\n",
    "    h1 = y_true_anchor_boxes[...,3]\n",
    "    x2 = pred_xy[...,0]\n",
    "    y2 = pred_xy[...,1]\n",
    "    w2 = pred_wh[...,0]\n",
    "    h2 = pred_wh[...,1]\n",
    "    iou = cal_iou(x1, y1, w1, h1, x2, y2, w2, h2)\n",
    "    iou = K.expand_dims(iou, -1)\n",
    "    # 使用 IoU 计算置信度的 target\n",
    "    pred_conf = K.sigmoid(y_pred[...,4:5])\n",
    "    loss_conf_obj = config.OBJECT_LAMBDA * K.sum(detector_mask * K.square(iou - pred_conf)) / (n_objs + 1e-6)\n",
    "    # 3.2、不包含目标的预测的bounding box置信度损失\n",
    "    # 预测值bounding box的xmin, ymin, xmax, ymax\n",
    "    pred_xy = K.expand_dims(pred_xy, 4)  # shape(batch_size, GRID_W, GRID_H, n_anchors, 1, 2)\n",
    "    pred_wh = K.expand_dims(pred_wh, 4)\n",
    "    pred_wh_half = pred_wh / 2.\n",
    "    pred_min = pred_xy - pred_wh_half\n",
    "    pred_max = pred_xy + pred_wh_half\n",
    "    # 标记值bounding box的xmin, ymin, xmax, ymax\n",
    "    true_boxes_shape = K.int_shape(y_true_boxes_all)\n",
    "    true_boxes_grid = K.reshape(y_true_boxes_all, [true_boxes_shape[0], 1, 1, 1, true_boxes_shape[1], true_boxes_shape[2]])\n",
    "    true_xy = true_boxes_grid[...,0:2]  # shape(batch_size, 1, 1, 1, max_annotation, 2)\n",
    "    true_wh = true_boxes_grid[...,2:4]  # shape(batch_size, 1, 1, 1, max_annotation, 2)\n",
    "    true_wh_half = true_wh * 0.5\n",
    "    true_min = true_xy - true_wh_half\n",
    "    true_max = true_xy + true_wh_half\n",
    "    # 计算每一个预测的box与所有标记的box的IoU，找出最大的IOU，如果小于阈值(0.6,并且不负责GT，根据1 - detector_mask)，该预测的box就加入noobj，计算置信度损失\n",
    "    intersect_min = K.maximum(pred_min, true_min)  # shape(batch_size, GRID_W, GRID_H, n_anchors, max_annotation, 2)\n",
    "    intersect_max = K.minimum(pred_max, true_max)  # shape(batch_size, GRID_W, GRID_H, n_anchors, max_annotation, 2)\n",
    "    intersect_wh = K.maximum(intersect_max - intersect_min, 0.)  # shape(batch_size, GRID_W, GRID_H, n_anchors, max_annotation, 1)\n",
    "    intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]  # shape(batch_size, GRID_W, GRID_H, n_anchors, max_annotation, 1)\n",
    "    pred_area = pred_wh[..., 0] * pred_wh[..., 1]  # shape(batch_size, GRID_W, GRID_H, n_anchors, 1, 1)\n",
    "    true_area = true_wh[..., 0] * true_wh[..., 1]  # shape(batch_size, GRID_W, GRID_H, n_anchors, max_annotation, 1)\n",
    "    union_area = pred_area + true_area - intersect_area\n",
    "    iou_score = intersect_area / union_area  # shape(batch_size, GRID_W, GRID_H, n_anchors, max_annotation, 1)\n",
    "    best_iou = K.max(iou_score, axis=4)  # Best IOU scores.\n",
    "    best_iou = K.expand_dims(best_iou)  # shape(batch_size, GRID_W, GRID_H, n_anchors, 1)\n",
    "    # 计算无目标损失\n",
    "    no_object_detection = K.cast(best_iou < 0.6, K.dtype(best_iou))\n",
    "    noobj_mask = no_object_detection * (1 - detector_mask)\n",
    "    n_noobj  = K.sum(tf.cast(noobj_mask  > 0.0, tf.float32))\n",
    "    loss_conf_noobj =  config.NOOBJECT_LAMBDA * K.sum(noobj_mask * K.square(-pred_conf)) / (n_noobj + 1e-6)\n",
    "    # 4、三种损失汇总\n",
    "    loss_conf = loss_conf_noobj + loss_conf_obj\n",
    "    loss = loss_conf + loss_cls + loss_coord\n",
    "    sub_loss = [loss_conf, loss_cls, loss_coord]\n",
    "    return loss, sub_loss\n",
    "\n",
    "def save_best_weights(model, name, val_loss_avg):\n",
    "    \"\"\"\n",
    "    保存最好的权重\n",
    "    \"\"\"\n",
    "    name = name + '_' + str(val_loss_avg) + '.h5'\n",
    "    path_name = os.path.join('weight/', name)\n",
    "    model.save_weights(path_name)\n",
    "    return path_name\n",
    "\n",
    "def log_loss(loss, val_loss, step):\n",
    "    tf.summary.scalar('loss', loss, step)\n",
    "    tf.summary.scalar('val_loss', val_loss, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014921,
     "end_time": "2021-01-15T06:42:48.324682",
     "exception": false,
     "start_time": "2021-01-15T06:42:48.309761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 6、开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T06:42:48.363687Z",
     "iopub.status.busy": "2021-01-15T06:42:48.362913Z",
     "iopub.status.idle": "2021-01-15T06:44:49.440553Z",
     "shell.execute_reply": "2021-01-15T06:44:49.439104Z"
    },
    "papermill": {
     "duration": 121.101215,
     "end_time": "2021-01-15T06:44:49.440669",
     "exception": false,
     "start_time": "2021-01-15T06:42:48.339454",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "data_dir = '../input/voc-2007-2012/VOCdevkit'\n",
    "val_set = {'VOC2012':['val']}\n",
    "train_set = {'VOC2007':['train', 'val', 'test'], 'VOC2012':['train']}\n",
    "\n",
    "batch_size = 6\n",
    "num_epochs = 200\n",
    "num_iterations = 30\n",
    "train_name = 'training_1'\n",
    "cfg = Config()\n",
    "model_weight_path = '../input/voc-2007-2012/weight/yolo.weights'\n",
    "n_progress = 20  # 进度条份数\n",
    "\n",
    "# log（tensorboard）\n",
    "summery_writer = tf.summary.create_file_writer(os.path.join('logs/', train_name), flush_millis=20000)\n",
    "summery_writer.set_as_default()\n",
    "\n",
    "dataset_train = OB_tensor_slices_dataset(data_dir, train_set, batch_size, cfg, shuffle=True)\n",
    "dataset_val = OB_tensor_slices_dataset(data_dir, val_set, batch_size, cfg, shuffle=False)\n",
    "len_batches_train = tf.data.experimental.cardinality(dataset_train).numpy()\n",
    "len_batches_val = tf.data.experimental.cardinality(dataset_val).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T06:44:49.475273Z",
     "iopub.status.busy": "2021-01-15T06:44:49.474755Z",
     "iopub.status.idle": "2021-01-15T06:44:55.809455Z",
     "shell.execute_reply": "2021-01-15T06:44:55.810263Z"
    },
    "papermill": {
     "duration": 6.354374,
     "end_time": "2021-01-15T06:44:55.810467",
     "exception": false,
     "start_time": "2021-01-15T06:44:49.456093",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 416, 416, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 416, 416, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm_1 (BatchNormalization)     (None, 416, 416, 32) 128         conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 416, 416, 32) 0           norm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 208, 208, 32) 0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 208, 208, 64) 18432       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_2 (BatchNormalization)     (None, 208, 208, 64) 256         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 208, 208, 64) 0           norm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 104, 104, 64) 0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, 104, 104, 128 73728       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_3 (BatchNormalization)     (None, 104, 104, 128 512         conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 104, 104, 128 0           norm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, 104, 104, 64) 8192        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_4 (BatchNormalization)     (None, 104, 104, 64) 256         conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 104, 104, 64) 0           norm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_5 (Conv2D)                 (None, 104, 104, 128 73728       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_5 (BatchNormalization)     (None, 104, 104, 128 512         conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 104, 104, 128 0           norm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 52, 52, 128)  0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_6 (Conv2D)                 (None, 52, 52, 256)  294912      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_6 (BatchNormalization)     (None, 52, 52, 256)  1024        conv_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 52, 52, 256)  0           norm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_7 (Conv2D)                 (None, 52, 52, 128)  32768       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_7 (BatchNormalization)     (None, 52, 52, 128)  512         conv_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 52, 52, 128)  0           norm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_8 (Conv2D)                 (None, 52, 52, 256)  294912      leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_8 (BatchNormalization)     (None, 52, 52, 256)  1024        conv_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 52, 52, 256)  0           norm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 26, 26, 256)  0           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_9 (Conv2D)                 (None, 26, 26, 512)  1179648     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_9 (BatchNormalization)     (None, 26, 26, 512)  2048        conv_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 26, 26, 512)  0           norm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_10 (Conv2D)                (None, 26, 26, 256)  131072      leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_10 (BatchNormalization)    (None, 26, 26, 256)  1024        conv_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 26, 26, 256)  0           norm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_11 (Conv2D)                (None, 26, 26, 512)  1179648     leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_11 (BatchNormalization)    (None, 26, 26, 512)  2048        conv_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 26, 26, 512)  0           norm_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_12 (Conv2D)                (None, 26, 26, 256)  131072      leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_12 (BatchNormalization)    (None, 26, 26, 256)  1024        conv_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 26, 26, 256)  0           norm_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_13 (Conv2D)                (None, 26, 26, 512)  1179648     leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_13 (BatchNormalization)    (None, 26, 26, 512)  2048        conv_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 26, 26, 512)  0           norm_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 13, 13, 512)  0           leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_14 (Conv2D)                (None, 13, 13, 1024) 4718592     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_14 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_15 (Conv2D)                (None, 13, 13, 512)  524288      leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_15 (BatchNormalization)    (None, 13, 13, 512)  2048        conv_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 13, 13, 512)  0           norm_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_16 (Conv2D)                (None, 13, 13, 1024) 4718592     leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_16 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_17 (Conv2D)                (None, 13, 13, 512)  524288      leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_17 (BatchNormalization)    (None, 13, 13, 512)  2048        conv_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 13, 13, 512)  0           norm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_18 (Conv2D)                (None, 13, 13, 1024) 4718592     leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_18 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_19 (Conv2D)                (None, 13, 13, 1024) 9437184     leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_19 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_21 (Conv2D)                (None, 26, 26, 64)   32768       leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm_21 (BatchNormalization)    (None, 26, 26, 64)   256         conv_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_20 (Conv2D)                (None, 13, 13, 1024) 9437184     leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 26, 26, 64)   0           norm_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm_20 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 13, 13, 256)  0           leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 13, 13, 1280) 0           lambda[0][0]                     \n",
      "                                                                 leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_22 (Conv2D)                (None, 13, 13, 1024) 11796480    concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "norm_22 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_23 (Conv2D)                (None, 13, 13, 125)  128125      leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 13, 13, 5, 25 0           conv_23[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 50,676,061\n",
      "Trainable params: 50,655,389\n",
      "Non-trainable params: 20,672\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 准备模型\n",
    "weight_reader = WeightReader(model_weight_path)\n",
    "model = yolo(cfg)\n",
    "weight_reader.load_weights(model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T06:44:55.904317Z",
     "iopub.status.busy": "2021-01-15T06:44:55.902523Z",
     "iopub.status.idle": "2021-01-15T06:44:55.904964Z",
     "shell.execute_reply": "2021-01-15T06:44:55.905865Z"
    },
    "papermill": {
     "duration": 0.027141,
     "end_time": "2021-01-15T06:44:55.906044",
     "exception": false,
     "start_time": "2021-01-15T06:44:55.878903",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 准备训练\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "best_val_loss = 1e6\n",
    "initial_learning_rate = 2e-5\n",
    "decay_epochs = 30 * num_iterations\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=decay_epochs,\n",
    "    decay_rate=0.5,\n",
    "    staircase=True\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "train_layers = ['conv_22', 'norm_22', 'conv_23']\n",
    "train_vars = []\n",
    "for name in train_layers:\n",
    "     train_vars += model.get_layer(name).trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T06:44:55.985169Z",
     "iopub.status.busy": "2021-01-15T06:44:55.984311Z",
     "iopub.status.idle": "2021-01-15T07:05:19.462057Z",
     "shell.execute_reply": "2021-01-15T07:05:19.461562Z"
    },
    "papermill": {
     "duration": 1223.527256,
     "end_time": "2021-01-15T07:05:19.462160",
     "exception": false,
     "start_time": "2021-01-15T06:44:55.934904",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 :\n",
      "----------------\n",
      "find better model for train\n",
      " \n",
      "train_loss=491.160 (conf=462.025, class=0.951, coords=28.183), val_loss=29.697 (conf=3.765, class=0.948, coords=24.984)\n",
      "\n",
      "Epoch 1 :\n",
      "----------------\n",
      "find better model for train\n",
      " \n",
      "train_loss=339.085 (conf=311.479, class=0.948, coords=26.657), val_loss=29.537 (conf=3.622, class=0.947, coords=24.968)\n",
      "\n",
      "Epoch 2 :\n",
      "---------------- \n",
      "train_loss=14495.428 (conf=14464.901, class=0.959, coords=29.568), val_loss=29.384 (conf=3.473, class=0.947, coords=24.964)\n",
      "\n",
      "Epoch 3 :\n",
      "----------------\n",
      "find better model for train\n",
      " \n",
      "train_loss=254.898 (conf=225.938, class=0.947, coords=28.012), val_loss=29.263 (conf=3.353, class=0.946, coords=24.964)\n",
      "\n",
      "Epoch 4 :\n",
      "---------------- \n",
      "train_loss=775.003 (conf=746.558, class=0.947, coords=27.498), val_loss=29.138 (conf=3.250, class=0.945, coords=24.943)\n",
      "\n",
      "Epoch 5 :\n",
      "---------------- \n",
      "train_loss=595.096 (conf=567.186, class=0.941, coords=26.969), val_loss=29.068 (conf=3.181, class=0.945, coords=24.942)\n",
      "\n",
      "Epoch 6 :\n",
      "----------------\n",
      "find better model for train\n",
      " \n",
      "train_loss=63.753 (conf=37.378, class=0.943, coords=25.432), val_loss=28.989 (conf=3.123, class=0.944, coords=24.922)\n",
      "\n",
      "Epoch 7 :\n",
      "---------------- \n",
      "train_loss=169.763 (conf=143.211, class=0.934, coords=25.618), val_loss=28.919 (conf=3.068, class=0.943, coords=24.908)\n",
      "\n",
      "Epoch 8 :\n",
      "---------------- \n",
      "train_loss=85.386 (conf=56.946, class=0.932, coords=27.508), val_loss=28.839 (conf=2.973, class=0.944, coords=24.922)\n",
      "\n",
      "Epoch 9 :\n",
      "---------------- \n",
      "train_loss=276.478 (conf=249.879, class=0.927, coords=25.672), val_loss=28.768 (conf=2.889, class=0.945, coords=24.934)\n",
      "\n",
      "Epoch 10 :\n",
      "---------------- \n",
      "train_loss=574.218 (conf=546.270, class=0.935, coords=27.013), val_loss=28.767 (conf=2.874, class=0.946, coords=24.947)\n",
      "\n",
      "Epoch 11 :\n",
      "---------------- \n",
      "train_loss=71.697 (conf=43.072, class=0.919, coords=27.706), val_loss=28.755 (conf=2.875, class=0.945, coords=24.935)\n",
      "\n",
      "Epoch 12 :\n",
      "---------------- \n",
      "train_loss=2971.968 (conf=2942.064, class=0.926, coords=28.978), val_loss=28.727 (conf=2.815, class=0.949, coords=24.964)\n",
      "\n",
      "Epoch 13 :\n",
      "----------------\n",
      "find better model for train\n",
      " \n",
      "train_loss=44.142 (conf=14.557, class=0.919, coords=28.666), val_loss=28.727 (conf=2.796, class=0.950, coords=24.981)\n",
      "\n",
      "Epoch 14 :\n",
      "---------------- \n",
      "train_loss=334.424 (conf=303.998, class=0.924, coords=29.502), val_loss=28.691 (conf=2.777, class=0.950, coords=24.964)\n",
      "\n",
      "Epoch 15 :\n",
      "---------------- \n",
      "train_loss=142.421 (conf=111.942, class=0.915, coords=29.564), val_loss=28.643 (conf=2.748, class=0.950, coords=24.944)\n",
      "\n",
      "Epoch 16 :\n",
      "---------------- \n",
      "train_loss=216.095 (conf=189.365, class=0.920, coords=25.811), val_loss=28.606 (conf=2.736, class=0.952, coords=24.918)\n",
      "\n",
      "Epoch 17 :\n",
      "---------------- \n",
      "train_loss=1542.488 (conf=1511.904, class=0.908, coords=29.676), val_loss=28.570 (conf=2.703, class=0.955, coords=24.913)\n",
      "\n",
      "Epoch 18 :\n",
      "---------------- \n",
      "train_loss=272.493 (conf=244.047, class=0.905, coords=27.541), val_loss=28.606 (conf=2.749, class=0.958, coords=24.899)\n",
      "\n",
      "Epoch 19 :\n",
      "---------------- \n",
      "train_loss=53.004 (conf=22.782, class=0.898, coords=29.324), val_loss=28.603 (conf=2.756, class=0.958, coords=24.888)\n",
      "\n",
      "Epoch 20 :\n",
      "---------------- \n",
      "train_loss=458.911 (conf=429.484, class=0.903, coords=28.524), val_loss=28.585 (conf=2.755, class=0.961, coords=24.868)\n",
      "\n",
      "Epoch 21 :\n",
      "----------------\n",
      "find better model for train\n",
      " \n",
      "train_loss=39.943 (conf=11.412, class=0.909, coords=27.622), val_loss=28.531 (conf=2.728, class=0.961, coords=24.842)\n",
      "\n",
      "Epoch 22 :\n",
      "----------------\n",
      "find better model for train\n",
      " \n",
      "train_loss=36.147 (conf=6.770, class=0.891, coords=28.487), val_loss=28.499 (conf=2.717, class=0.960, coords=24.822)\n",
      "\n",
      "Epoch 23 :\n",
      "---------------- \n",
      "train_loss=51.048 (conf=24.145, class=0.881, coords=26.022), val_loss=28.484 (conf=2.729, class=0.956, coords=24.798)\n",
      "\n",
      "Epoch 24 :\n",
      "---------------- \n",
      "train_loss=1776.221 (conf=1749.305, class=0.885, coords=26.030), val_loss=28.739 (conf=2.928, class=0.958, coords=24.853)\n",
      "\n",
      "Epoch 25 :\n",
      "---------------- \n",
      "train_loss=8667.245 (conf=8638.783, class=0.882, coords=27.581), val_loss=30.153 (conf=4.179, class=0.964, coords=25.010)\n",
      "\n",
      "Epoch 26 :\n",
      "---------------- \n",
      "train_loss=279.536 (conf=253.282, class=0.897, coords=25.356), val_loss=33.384 (conf=7.330, class=0.964, coords=25.091)\n",
      "\n",
      "Epoch 27 :\n",
      "---------------- \n",
      "train_loss=384.696 (conf=355.140, class=0.877, coords=28.679), val_loss=31.706 (conf=5.701, class=0.969, coords=25.037)\n",
      "\n",
      "Epoch 28 :\n",
      "---------------- \n",
      "train_loss=57.403 (conf=28.194, class=0.876, coords=28.334), val_loss=30.964 (conf=4.997, class=0.968, coords=24.999)\n",
      "\n",
      "Epoch 29 :\n",
      "---------------- \n",
      "train_loss=629.347 (conf=601.309, class=0.867, coords=27.172), val_loss=30.466 (conf=4.525, class=0.967, coords=24.973)\n",
      "\n",
      "Epoch 30 :\n",
      "---------------- \n",
      "train_loss=44.062 (conf=16.271, class=0.889, coords=26.902), val_loss=29.671 (conf=3.807, class=0.964, coords=24.901)\n",
      "\n",
      "Epoch 31 :\n",
      "---------------- \n",
      "train_loss=510.942 (conf=481.496, class=0.856, coords=28.590), val_loss=29.273 (conf=3.431, class=0.967, coords=24.876)\n",
      "\n",
      "Epoch 32 :\n",
      "---------------- \n",
      "train_loss=56.496 (conf=29.674, class=0.876, coords=25.947), val_loss=29.171 (conf=3.321, class=0.970, coords=24.879)\n",
      "\n",
      "Epoch 33 :\n",
      "---------------- \n",
      "train_loss=41.681 (conf=14.528, class=0.873, coords=26.280), val_loss=29.123 (conf=3.288, class=0.969, coords=24.866)\n",
      "\n",
      "Epoch 34 :\n",
      "---------------- \n",
      "train_loss=8894.740 (conf=8866.500, class=0.874, coords=27.365), val_loss=28.632 (conf=2.863, class=0.972, coords=24.797)\n",
      "\n",
      "Epoch 35 :\n",
      "---------------- \n",
      "train_loss=131.825 (conf=104.285, class=0.854, coords=26.686), val_loss=28.504 (conf=2.771, class=0.972, coords=24.761)\n",
      "\n",
      "Epoch 36 :\n",
      "---------------- \n",
      "train_loss=2422.368 (conf=2396.354, class=0.839, coords=25.174), val_loss=28.476 (conf=2.766, class=0.972, coords=24.737)\n",
      "\n",
      "Epoch 37 :\n",
      "---------------- \n",
      "train_loss=188.587 (conf=156.946, class=0.860, coords=30.781), val_loss=28.400 (conf=2.714, class=0.971, coords=24.715)\n",
      "\n",
      "Epoch 38 :\n",
      "---------------- \n",
      "train_loss=497.993 (conf=465.970, class=0.848, coords=31.174), val_loss=28.347 (conf=2.681, class=0.970, coords=24.697)\n",
      "\n",
      "Epoch 39 :\n",
      "---------------- \n",
      "train_loss=136.844 (conf=108.800, class=0.856, coords=27.188), val_loss=28.301 (conf=2.673, class=0.972, coords=24.656)\n",
      "\n",
      "Epoch 40 :\n",
      "---------------- \n",
      "train_loss=218.554 (conf=192.358, class=0.852, coords=25.345), val_loss=28.291 (conf=2.664, class=0.975, coords=24.652)\n",
      "\n",
      "Epoch 41 :\n",
      "---------------- \n",
      "train_loss=47.654 (conf=17.641, class=0.851, coords=29.162), val_loss=28.238 (conf=2.659, class=0.972, coords=24.607)\n",
      "\n",
      "Epoch 42 :\n",
      "---------------- \n",
      "train_loss=460.452 (conf=433.837, class=0.853, coords=25.762), val_loss=28.235 (conf=2.647, class=0.975, coords=24.613)\n",
      "\n",
      "Epoch 43 :\n",
      "---------------- \n",
      "train_loss=2472.532 (conf=2443.985, class=0.844, coords=27.702), val_loss=28.205 (conf=2.649, class=0.973, coords=24.584)\n",
      "\n",
      "Epoch 44 :\n",
      "---------------- \n",
      "train_loss=54.266 (conf=24.919, class=0.826, coords=28.520), val_loss=28.177 (conf=2.632, class=0.972, coords=24.573)\n",
      "\n",
      "Epoch 45 :\n",
      "---------------- \n",
      "train_loss=50.642 (conf=22.885, class=0.853, coords=26.904), val_loss=28.169 (conf=2.640, class=0.971, coords=24.558)\n",
      "\n",
      "Epoch 46 :\n",
      "---------------- \n",
      "train_loss=60.845 (conf=33.405, class=0.836, coords=26.605), val_loss=28.081 (conf=2.608, class=0.971, coords=24.502)\n",
      "\n",
      "Epoch 47 :\n",
      "---------------- \n",
      "train_loss=290.799 (conf=261.092, class=0.842, coords=28.865), val_loss=28.060 (conf=2.599, class=0.971, coords=24.489)\n",
      "\n",
      "Epoch 48 :\n",
      "---------------- \n",
      "train_loss=92.708 (conf=64.317, class=0.844, coords=27.547), val_loss=28.068 (conf=2.614, class=0.972, coords=24.481)\n",
      "\n",
      "Epoch 49 :\n",
      "---------------- \n",
      "train_loss=1143.235 (conf=1116.579, class=0.840, coords=25.816), val_loss=28.027 (conf=2.613, class=0.969, coords=24.444)\n",
      "\n",
      "Epoch 50 :\n",
      "---------------- \n",
      "train_loss=44.870 (conf=16.076, class=0.837, coords=27.957), val_loss=27.958 (conf=2.603, class=0.966, coords=24.389)\n",
      "\n",
      "Epoch 51 :\n",
      "---------------- \n",
      "train_loss=114.655 (conf=86.665, class=0.848, coords=27.142), val_loss=27.932 (conf=2.595, class=0.967, coords=24.370)\n",
      "\n",
      "Epoch 52 :\n",
      "---------------- \n",
      "train_loss=493.570 (conf=465.449, class=0.836, coords=27.284), val_loss=27.923 (conf=2.588, class=0.968, coords=24.367)\n",
      "\n",
      "Epoch 53 :\n",
      "---------------- \n",
      "train_loss=103.301 (conf=75.931, class=0.814, coords=26.557), val_loss=27.906 (conf=2.595, class=0.967, coords=24.344)\n",
      "\n",
      "Epoch 54 :\n",
      "---------------- \n",
      "train_loss=45.547 (conf=15.040, class=0.839, coords=29.668), val_loss=27.885 (conf=2.573, class=0.966, coords=24.347)\n",
      "\n",
      "Epoch 55 :\n",
      "---------------- \n",
      "train_loss=37.991 (conf=10.110, class=0.816, coords=27.065), val_loss=27.899 (conf=2.568, class=0.968, coords=24.363)\n",
      "\n",
      "Epoch 56 :\n",
      "---------------- \n",
      "train_loss=283.663 (conf=257.793, class=0.798, coords=25.072), val_loss=27.851 (conf=2.567, class=0.963, coords=24.321)\n",
      "\n",
      "Epoch 57 :\n",
      "---------------- \n",
      "train_loss=41.329 (conf=14.901, class=0.816, coords=25.612), val_loss=27.825 (conf=2.555, class=0.963, coords=24.307)\n",
      "\n",
      "Epoch 58 :\n",
      "---------------- \n",
      "train_loss=75.349 (conf=48.361, class=0.813, coords=26.174), val_loss=27.808 (conf=2.552, class=0.962, coords=24.294)\n",
      "\n",
      "Epoch 59 :\n",
      "---------------- \n",
      "train_loss=181.158 (conf=152.640, class=0.822, coords=27.696), val_loss=27.804 (conf=2.554, class=0.962, coords=24.288)\n",
      "\n",
      "Epoch 60 :\n",
      "---------------- \n",
      "train_loss=42.428 (conf=12.009, class=0.811, coords=29.608), val_loss=27.797 (conf=2.561, class=0.968, coords=24.268)\n",
      "\n",
      "Epoch 61 :\n",
      "---------------- \n",
      "train_loss=70.933 (conf=42.582, class=0.813, coords=27.538), val_loss=27.748 (conf=2.557, class=0.967, coords=24.224)\n",
      "\n",
      "Epoch 62 :\n",
      "---------------- \n",
      "train_loss=2677.796 (conf=2645.969, class=0.808, coords=31.019), val_loss=27.629 (conf=2.495, class=0.967, coords=24.167)\n",
      "\n",
      "Epoch 63 :\n",
      "---------------- \n",
      "train_loss=283.391 (conf=253.370, class=0.803, coords=29.218), val_loss=27.644 (conf=2.501, class=0.966, coords=24.177)\n",
      "\n",
      "Epoch 64 :\n",
      "---------------- \n",
      "train_loss=54.339 (conf=26.455, class=0.801, coords=27.082), val_loss=27.629 (conf=2.499, class=0.967, coords=24.164)\n",
      "\n",
      "Epoch 65 :\n",
      "---------------- \n",
      "train_loss=42.499 (conf=13.975, class=0.798, coords=27.726), val_loss=27.592 (conf=2.495, class=0.964, coords=24.132)\n",
      "\n",
      "Epoch 66 :\n",
      "---------------- \n",
      "train_loss=680.109 (conf=650.493, class=0.813, coords=28.802), val_loss=27.576 (conf=2.491, class=0.962, coords=24.123)\n",
      "\n",
      "Epoch 67 :\n",
      "---------------- \n",
      "train_loss=79.871 (conf=53.402, class=0.794, coords=25.675), val_loss=27.597 (conf=2.492, class=0.962, coords=24.143)\n",
      "\n",
      "Epoch 68 :\n",
      "---------------- \n",
      "train_loss=86.018 (conf=59.356, class=0.818, coords=25.844), val_loss=27.508 (conf=2.486, class=0.958, coords=24.064)\n",
      "\n",
      "Epoch 69 :\n",
      "---------------- \n",
      "train_loss=149.654 (conf=119.886, class=0.794, coords=28.974), val_loss=27.501 (conf=2.495, class=0.956, coords=24.049)\n",
      "\n",
      "Epoch 70 :\n",
      "---------------- \n",
      "train_loss=71.593 (conf=42.836, class=0.797, coords=27.960), val_loss=27.491 (conf=2.494, class=0.954, coords=24.043)\n",
      "\n",
      "Epoch 71 :\n",
      "---------------- \n",
      "train_loss=98.961 (conf=68.797, class=0.802, coords=29.363), val_loss=27.479 (conf=2.501, class=0.953, coords=24.024)\n",
      "\n",
      "Epoch 72 :\n",
      "---------------- \n",
      "train_loss=43.346 (conf=17.721, class=0.778, coords=24.846), val_loss=27.468 (conf=2.484, class=0.956, coords=24.028)\n",
      "\n",
      "Epoch 73 :\n",
      "---------------- \n",
      "train_loss=90.757 (conf=62.221, class=0.787, coords=27.749), val_loss=27.454 (conf=2.467, class=0.958, coords=24.029)\n",
      "\n",
      "Epoch 74 :\n",
      "---------------- \n",
      "train_loss=85.359 (conf=58.003, class=0.786, coords=26.569), val_loss=27.415 (conf=2.480, class=0.957, coords=23.979)\n",
      "\n",
      "Epoch 75 :\n",
      "---------------- \n",
      "train_loss=95.521 (conf=64.237, class=0.788, coords=30.497), val_loss=27.447 (conf=2.493, class=0.960, coords=23.994)\n",
      "\n",
      "Epoch 76 :\n",
      "---------------- \n",
      "train_loss=129.750 (conf=103.258, class=0.797, coords=25.695), val_loss=27.394 (conf=2.479, class=0.957, coords=23.958)\n",
      "\n",
      "Epoch 77 :\n",
      "---------------- \n",
      "train_loss=45.540 (conf=17.145, class=0.780, coords=27.615), val_loss=27.379 (conf=2.485, class=0.959, coords=23.935)\n",
      "\n",
      "Epoch 78 :\n",
      "---------------- \n",
      "train_loss=54.248 (conf=23.068, class=0.780, coords=30.400), val_loss=27.354 (conf=2.482, class=0.958, coords=23.914)\n",
      "\n",
      "Epoch 79 :\n",
      "---------------- \n",
      "train_loss=42.106 (conf=13.625, class=0.796, coords=27.685), val_loss=27.355 (conf=2.486, class=0.961, coords=23.908)\n",
      "\n",
      "Epoch 80 :\n",
      "---------------- \n",
      "train_loss=49.023 (conf=21.962, class=0.786, coords=26.274), val_loss=27.355 (conf=2.489, class=0.962, coords=23.903)\n",
      "\n",
      "Epoch 81 :\n",
      "---------------- \n",
      "train_loss=43.195 (conf=15.804, class=0.790, coords=26.602), val_loss=27.349 (conf=2.477, class=0.963, coords=23.909)\n",
      "\n",
      "Epoch 82 :\n",
      "---------------- \n",
      "train_loss=91.662 (conf=65.335, class=0.753, coords=25.574), val_loss=27.286 (conf=2.453, class=0.961, coords=23.872)\n",
      "\n",
      "Epoch 83 :\n",
      "---------------- \n",
      "train_loss=39.454 (conf=11.075, class=0.783, coords=27.595), val_loss=27.298 (conf=2.464, class=0.958, coords=23.877)\n",
      "\n",
      "Epoch 84 :\n",
      "---------------- \n",
      "train_loss=41313.488 (conf=41285.770, class=0.772, coords=26.942), val_loss=26.954 (conf=2.153, class=0.953, coords=23.847)\n",
      "\n",
      "Epoch 85 :\n",
      "---------------- \n",
      "train_loss=1092.521 (conf=1061.368, class=0.750, coords=30.404), val_loss=27.190 (conf=2.003, class=0.950, coords=24.238)\n",
      "\n",
      "Epoch 86 :\n",
      "---------------- \n",
      "train_loss=50.298 (conf=25.225, class=0.762, coords=24.311), val_loss=27.040 (conf=2.024, class=0.949, coords=24.067)\n",
      "\n",
      "Epoch 87 :\n",
      "---------------- \n",
      "train_loss=42.592 (conf=16.359, class=0.752, coords=25.481), val_loss=26.986 (conf=2.034, class=0.952, coords=24.000)\n",
      "\n",
      "Epoch 88 :\n",
      "---------------- \n",
      "train_loss=40.881 (conf=13.206, class=0.769, coords=26.907), val_loss=26.938 (conf=2.056, class=0.952, coords=23.930)\n",
      "\n",
      "Epoch 89 :\n",
      "---------------- \n",
      "train_loss=108.903 (conf=81.203, class=0.755, coords=26.946), val_loss=26.907 (conf=2.058, class=0.954, coords=23.895)\n",
      "\n",
      "Epoch 90 :\n",
      "---------------- \n",
      "train_loss=48.949 (conf=20.139, class=0.780, coords=28.031), val_loss=26.843 (conf=2.070, class=0.953, coords=23.821)\n",
      "\n",
      "Epoch 91 :\n",
      "---------------- \n",
      "train_loss=127.413 (conf=98.782, class=0.760, coords=27.871), val_loss=26.805 (conf=2.087, class=0.950, coords=23.768)\n",
      "\n",
      "Epoch 92 :\n",
      "---------------- \n",
      "train_loss=551.695 (conf=525.273, class=0.750, coords=25.672), val_loss=26.819 (conf=2.096, class=0.951, coords=23.772)\n",
      "\n",
      "Epoch 93 :\n",
      "---------------- \n",
      "train_loss=46.411 (conf=18.119, class=0.746, coords=27.546), val_loss=26.817 (conf=2.099, class=0.954, coords=23.764)\n",
      "\n",
      "Epoch 94 :\n",
      "---------------- \n",
      "train_loss=41.112 (conf=15.441, class=0.757, coords=24.915), val_loss=26.773 (conf=2.102, class=0.955, coords=23.717)\n",
      "\n",
      "Epoch 95 :\n",
      "---------------- \n",
      "train_loss=229.010 (conf=200.864, class=0.774, coords=27.372), val_loss=26.773 (conf=2.095, class=0.954, coords=23.723)\n",
      "\n",
      "Epoch 96 :\n",
      "----------------\n",
      "find better model for train\n",
      " \n",
      "train_loss=30.881 (conf=6.857, class=0.757, coords=23.267), val_loss=26.762 (conf=2.099, class=0.954, coords=23.708)\n",
      "\n",
      "Epoch 97 :\n",
      "---------------- \n",
      "train_loss=152.905 (conf=128.482, class=0.760, coords=23.663), val_loss=26.766 (conf=2.098, class=0.952, coords=23.715)\n",
      "\n",
      "Epoch 98 :\n",
      "---------------- \n",
      "train_loss=48.679 (conf=20.893, class=0.742, coords=27.044), val_loss=26.762 (conf=2.088, class=0.954, coords=23.720)\n",
      "\n",
      "Epoch 99 :\n",
      "---------------- \n",
      "train_loss=120.163 (conf=93.846, class=0.722, coords=25.595), val_loss=26.744 (conf=2.073, class=0.955, coords=23.716)\n",
      "\n",
      "Epoch 100 :\n",
      "---------------- \n",
      "train_loss=642.438 (conf=615.638, class=0.740, coords=26.060), val_loss=26.695 (conf=2.085, class=0.951, coords=23.660)\n",
      "\n",
      "Epoch 101 :\n",
      "---------------- \n",
      "train_loss=39.005 (conf=11.168, class=0.744, coords=27.093), val_loss=26.643 (conf=2.087, class=0.950, coords=23.607)\n",
      "\n",
      "Epoch 102 :\n",
      "---------------- \n",
      "train_loss=51.674 (conf=25.879, class=0.761, coords=25.033), val_loss=26.627 (conf=2.088, class=0.949, coords=23.590)\n",
      "\n",
      "Epoch 103 :\n",
      "---------------- \n",
      "train_loss=435.250 (conf=407.592, class=0.745, coords=26.913), val_loss=26.604 (conf=2.081, class=0.949, coords=23.574)\n",
      "\n",
      "Epoch 104 :\n",
      "---------------- \n",
      "train_loss=113.222 (conf=86.164, class=0.746, coords=26.312), val_loss=26.595 (conf=2.075, class=0.948, coords=23.572)\n",
      "\n",
      "Epoch 105 :\n",
      "---------------- \n",
      "train_loss=287.658 (conf=260.400, class=0.765, coords=26.493), val_loss=26.526 (conf=2.075, class=0.947, coords=23.504)\n",
      "\n",
      "Epoch 106 :\n",
      "---------------- \n",
      "train_loss=98.621 (conf=75.233, class=0.762, coords=22.625), val_loss=26.488 (conf=2.065, class=0.947, coords=23.475)\n",
      "\n",
      "Epoch 107 :\n",
      "---------------- \n",
      "train_loss=37.551 (conf=8.618, class=0.732, coords=28.200), val_loss=26.444 (conf=2.075, class=0.944, coords=23.425)\n",
      "\n",
      "Epoch 108 :\n",
      "---------------- \n",
      "train_loss=35.891 (conf=7.643, class=0.716, coords=27.532), val_loss=26.449 (conf=2.064, class=0.945, coords=23.439)\n",
      "\n",
      "Epoch 109 :\n",
      "---------------- \n",
      "train_loss=88.577 (conf=59.465, class=0.742, coords=28.370), val_loss=26.422 (conf=2.065, class=0.944, coords=23.413)\n",
      "\n",
      "Epoch 110 :\n",
      "---------------- \n",
      "train_loss=44.168 (conf=13.922, class=0.724, coords=29.522), val_loss=26.414 (conf=2.049, class=0.948, coords=23.416)\n",
      "\n",
      "Epoch 111 :\n",
      "---------------- \n",
      "train_loss=31.875 (conf=5.576, class=0.745, coords=25.554), val_loss=26.428 (conf=2.040, class=0.953, coords=23.436)\n",
      "\n",
      "Epoch 112 :\n",
      "---------------- \n",
      "train_loss=174.309 (conf=147.979, class=0.722, coords=25.608), val_loss=26.396 (conf=2.055, class=0.946, coords=23.394)\n",
      "\n",
      "Epoch 113 :\n",
      "---------------- \n",
      "train_loss=37.351 (conf=11.039, class=0.722, coords=25.590), val_loss=26.374 (conf=2.047, class=0.946, coords=23.381)\n",
      "\n",
      "Epoch 114 :\n",
      "---------------- \n",
      "train_loss=70.379 (conf=40.992, class=0.738, coords=28.649), val_loss=26.350 (conf=2.050, class=0.942, coords=23.358)\n",
      "\n",
      "Epoch 115 :\n",
      "---------------- \n",
      "train_loss=45.235 (conf=13.010, class=0.746, coords=31.479), val_loss=26.334 (conf=2.042, class=0.944, coords=23.349)\n",
      "\n",
      "Epoch 116 :\n",
      "---------------- \n",
      "train_loss=1104.933 (conf=1077.994, class=0.720, coords=26.218), val_loss=26.325 (conf=2.038, class=0.945, coords=23.342)\n",
      "\n",
      "Epoch 117 :\n",
      "---------------- \n",
      "train_loss=300.562 (conf=273.438, class=0.706, coords=26.417), val_loss=26.288 (conf=2.025, class=0.946, coords=23.317)\n",
      "\n",
      "Epoch 118 :\n",
      "---------------- \n",
      "train_loss=551.871 (conf=527.947, class=0.729, coords=23.195), val_loss=26.303 (conf=2.023, class=0.947, coords=23.333)\n",
      "\n",
      "Epoch 119 :\n",
      "---------------- \n",
      "train_loss=60.801 (conf=32.902, class=0.733, coords=27.166), val_loss=26.280 (conf=2.026, class=0.946, coords=23.307)\n",
      "\n",
      "Epoch 120 :\n",
      "---------------- \n",
      "train_loss=157.570 (conf=132.205, class=0.719, coords=24.646), val_loss=26.261 (conf=2.029, class=0.941, coords=23.290)\n",
      "\n",
      "Epoch 121 :\n",
      "---------------- \n",
      "train_loss=951.832 (conf=924.061, class=0.700, coords=27.071), val_loss=26.300 (conf=2.015, class=0.941, coords=23.344)\n",
      "\n",
      "Epoch 122 :\n",
      "---------------- \n",
      "train_loss=45.963 (conf=21.908, class=0.717, coords=23.339), val_loss=26.268 (conf=2.019, class=0.937, coords=23.313)\n",
      "\n",
      "Epoch 123 :\n",
      "---------------- \n",
      "train_loss=43.388 (conf=16.675, class=0.719, coords=25.994), val_loss=26.266 (conf=2.015, class=0.936, coords=23.315)\n",
      "\n",
      "Epoch 124 :\n",
      "---------------- \n",
      "train_loss=296.101 (conf=268.606, class=0.705, coords=26.789), val_loss=26.264 (conf=2.009, class=0.938, coords=23.317)\n",
      "\n",
      "Epoch 125 :\n",
      "---------------- \n",
      "train_loss=69.269 (conf=41.045, class=0.682, coords=27.542), val_loss=26.250 (conf=2.005, class=0.936, coords=23.309)\n",
      "\n",
      "Epoch 126 :\n",
      "---------------- \n",
      "train_loss=41.745 (conf=14.479, class=0.707, coords=26.560), val_loss=26.235 (conf=1.995, class=0.936, coords=23.304)\n",
      "\n",
      "Epoch 127 :\n",
      "---------------- \n",
      "train_loss=134990.094 (conf=134959.078, class=0.697, coords=30.328), val_loss=26.224 (conf=2.129, class=0.934, coords=23.160)\n",
      "\n",
      "Epoch 128 :\n",
      "---------------- \n",
      "train_loss=67.683 (conf=41.966, class=0.701, coords=25.016), val_loss=26.427 (conf=2.305, class=0.932, coords=23.190)\n",
      "\n",
      "Epoch 129 :\n",
      "---------------- \n",
      "train_loss=49.707 (conf=23.261, class=0.712, coords=25.734), val_loss=26.331 (conf=2.224, class=0.931, coords=23.176)\n",
      "\n",
      "Epoch 130 :\n",
      "---------------- \n",
      "train_loss=36.775 (conf=7.902, class=0.736, coords=28.138), val_loss=26.256 (conf=2.169, class=0.926, coords=23.161)\n",
      "\n",
      "Epoch 131 :\n",
      "---------------- \n",
      "train_loss=3658.068 (conf=3629.462, class=0.704, coords=27.903), val_loss=26.180 (conf=2.130, class=0.925, coords=23.124)\n",
      "\n",
      "Epoch 132 :\n",
      "---------------- \n",
      "train_loss=116.128 (conf=88.708, class=0.720, coords=26.700), val_loss=26.130 (conf=2.111, class=0.922, coords=23.097)\n",
      "\n",
      "Epoch 133 :\n",
      "---------------- \n",
      "train_loss=3905.409 (conf=3880.620, class=0.703, coords=24.086), val_loss=26.086 (conf=2.101, class=0.920, coords=23.066)\n",
      "\n",
      "Epoch 134 :\n",
      "---------------- \n",
      "train_loss=388.599 (conf=361.618, class=0.697, coords=26.284), val_loss=26.063 (conf=2.090, class=0.924, coords=23.049)\n",
      "\n",
      "Epoch 135 :\n",
      "---------------- \n",
      "train_loss=56.401 (conf=27.557, class=0.678, coords=28.167), val_loss=26.054 (conf=2.082, class=0.931, coords=23.042)\n",
      "\n",
      "Epoch 136 :\n",
      "---------------- \n",
      "train_loss=2881.433 (conf=2854.677, class=0.693, coords=26.064), val_loss=26.031 (conf=2.066, class=0.932, coords=23.032)\n",
      "\n",
      "Epoch 137 :\n",
      "---------------- \n",
      "train_loss=40.173 (conf=15.504, class=0.695, coords=23.974), val_loss=25.999 (conf=2.061, class=0.929, coords=23.009)\n",
      "\n",
      "Epoch 138 :\n",
      "---------------- \n",
      "train_loss=12897.684 (conf=12869.752, class=0.711, coords=27.220), val_loss=25.968 (conf=2.045, class=0.923, coords=22.999)\n",
      "\n",
      "Epoch 139 :\n",
      "---------------- \n",
      "train_loss=91.617 (conf=64.300, class=0.710, coords=26.607), val_loss=25.942 (conf=2.047, class=0.922, coords=22.973)\n",
      "\n",
      "Epoch 140 :\n",
      "---------------- \n",
      "train_loss=42.752 (conf=12.309, class=0.667, coords=29.777), val_loss=25.975 (conf=2.051, class=0.922, coords=23.003)\n",
      "\n",
      "Epoch 141 :\n",
      "---------------- \n",
      "train_loss=72.098 (conf=45.790, class=0.685, coords=25.623), val_loss=25.999 (conf=2.038, class=0.923, coords=23.037)\n",
      "\n",
      "Epoch 142 :\n",
      "---------------- \n",
      "train_loss=32.667 (conf=9.085, class=0.672, coords=22.911), val_loss=25.970 (conf=2.035, class=0.927, coords=23.008)\n",
      "\n",
      "Epoch 143 :\n",
      "---------------- \n",
      "train_loss=38.856 (conf=13.647, class=0.686, coords=24.522), val_loss=25.962 (conf=2.039, class=0.920, coords=23.003)\n",
      "\n",
      "Epoch 144 :\n",
      "---------------- \n",
      "train_loss=39.780 (conf=12.991, class=0.670, coords=26.119), val_loss=25.941 (conf=2.038, class=0.918, coords=22.985)\n",
      "\n",
      "Epoch 145 :\n",
      "---------------- \n",
      "train_loss=180.488 (conf=154.655, class=0.692, coords=25.142), val_loss=25.918 (conf=2.042, class=0.917, coords=22.959)\n",
      "\n",
      "Epoch 146 :\n",
      "---------------- \n",
      "train_loss=38.080 (conf=11.278, class=0.692, coords=26.109), val_loss=25.891 (conf=2.034, class=0.919, coords=22.938)\n",
      "\n",
      "Epoch 147 :\n",
      "---------------- \n",
      "train_loss=44.239 (conf=15.157, class=0.710, coords=28.371), val_loss=25.890 (conf=2.038, class=0.912, coords=22.939)\n",
      "\n",
      "Epoch 148 :\n",
      "---------------- \n",
      "train_loss=35.425 (conf=8.743, class=0.662, coords=26.019), val_loss=25.816 (conf=2.037, class=0.912, coords=22.867)\n",
      "\n",
      "Epoch 149 :\n",
      "---------------- \n",
      "train_loss=60.563 (conf=34.525, class=0.685, coords=25.353), val_loss=25.822 (conf=2.037, class=0.911, coords=22.873)\n",
      "\n",
      "Epoch 150 :\n",
      "---------------- \n",
      "train_loss=813.281 (conf=786.420, class=0.661, coords=26.200), val_loss=25.805 (conf=2.039, class=0.910, coords=22.856)\n",
      "\n",
      "Epoch 151 :\n",
      "---------------- \n",
      "train_loss=52.691 (conf=28.441, class=0.689, coords=23.561), val_loss=25.787 (conf=2.030, class=0.912, coords=22.845)\n",
      "\n",
      "Epoch 152 :\n",
      "---------------- \n",
      "train_loss=39.358 (conf=12.273, class=0.663, coords=26.423), val_loss=25.781 (conf=2.022, class=0.909, coords=22.850)\n",
      "\n",
      "Epoch 153 :\n",
      "---------------- \n",
      "train_loss=40.706 (conf=14.553, class=0.693, coords=25.460), val_loss=25.801 (conf=2.023, class=0.914, coords=22.864)\n",
      "\n",
      "Epoch 154 :\n",
      "---------------- \n",
      "train_loss=32.349 (conf=7.710, class=0.681, coords=23.958), val_loss=25.783 (conf=2.023, class=0.913, coords=22.847)\n",
      "\n",
      "Epoch 155 :\n",
      "---------------- \n",
      "train_loss=65.056 (conf=39.675, class=0.678, coords=24.703), val_loss=25.737 (conf=2.025, class=0.913, coords=22.799)\n",
      "\n",
      "Epoch 156 :\n",
      "---------------- \n",
      "train_loss=55.900 (conf=29.427, class=0.676, coords=25.796), val_loss=25.689 (conf=2.022, class=0.911, coords=22.755)\n",
      "\n",
      "Epoch 157 :\n",
      "---------------- \n",
      "train_loss=236.728 (conf=209.877, class=0.689, coords=26.161), val_loss=25.678 (conf=2.021, class=0.907, coords=22.750)\n",
      "\n",
      "Epoch 158 :\n",
      "---------------- \n",
      "train_loss=72.726 (conf=47.819, class=0.668, coords=24.239), val_loss=25.656 (conf=2.032, class=0.904, coords=22.720)\n",
      "\n",
      "Epoch 159 :\n",
      "---------------- \n",
      "train_loss=45.620 (conf=16.406, class=0.671, coords=28.543), val_loss=25.623 (conf=2.031, class=0.905, coords=22.686)\n",
      "\n",
      "Epoch 160 :\n",
      "---------------- \n",
      "train_loss=14320.688 (conf=14292.191, class=0.679, coords=27.821), val_loss=25.637 (conf=2.072, class=0.907, coords=22.658)\n",
      "\n",
      "Epoch 161 :\n",
      "---------------- \n",
      "train_loss=185.695 (conf=158.806, class=0.659, coords=26.230), val_loss=25.638 (conf=2.081, class=0.905, coords=22.652)\n",
      "\n",
      "Epoch 162 :\n",
      "---------------- \n",
      "train_loss=92.857 (conf=67.116, class=0.667, coords=25.074), val_loss=25.608 (conf=2.066, class=0.903, coords=22.640)\n",
      "\n",
      "Epoch 163 :\n",
      "---------------- \n",
      "train_loss=137.364 (conf=107.627, class=0.651, coords=29.086), val_loss=25.594 (conf=2.058, class=0.902, coords=22.634)\n",
      "\n",
      "Epoch 164 :\n",
      "---------------- \n",
      "train_loss=31.702 (conf=6.207, class=0.632, coords=24.863), val_loss=25.582 (conf=2.043, class=0.906, coords=22.633)\n",
      "\n",
      "Epoch 165 :\n",
      "---------------- \n",
      "train_loss=92.388 (conf=68.566, class=0.681, coords=23.141), val_loss=25.562 (conf=2.049, class=0.902, coords=22.611)\n",
      "\n",
      "Epoch 166 :\n",
      "---------------- \n",
      "train_loss=38.609 (conf=9.728, class=0.665, coords=28.216), val_loss=25.553 (conf=2.043, class=0.899, coords=22.611)\n",
      "\n",
      "Epoch 167 :\n",
      "---------------- \n",
      "train_loss=432.104 (conf=409.565, class=0.657, coords=21.882), val_loss=25.521 (conf=2.046, class=0.895, coords=22.579)\n",
      "\n",
      "Epoch 168 :\n",
      "---------------- \n",
      "train_loss=44.010 (conf=14.547, class=0.661, coords=28.803), val_loss=25.527 (conf=2.037, class=0.898, coords=22.592)\n",
      "\n",
      "Epoch 169 :\n",
      "---------------- \n",
      "train_loss=41038.918 (conf=41013.551, class=0.691, coords=24.674), val_loss=25.811 (conf=2.292, class=0.899, coords=22.620)\n",
      "\n",
      "Epoch 170 :\n",
      "---------------- \n",
      "train_loss=35.167 (conf=8.295, class=0.632, coords=26.240), val_loss=26.051 (conf=2.460, class=0.902, coords=22.689)\n",
      "\n",
      "Epoch 171 :\n",
      "---------------- \n",
      "train_loss=272.731 (conf=248.561, class=0.648, coords=23.522), val_loss=26.044 (conf=2.409, class=0.900, coords=22.734)\n",
      "\n",
      "Epoch 172 :\n",
      "---------------- \n",
      "train_loss=4123586.250 (conf=4123557.750, class=0.652, coords=27.711), val_loss=25.786 (conf=1.833, class=0.900, coords=23.053)\n",
      "\n",
      "Epoch 173 :\n",
      "---------------- \n",
      "train_loss=81.301 (conf=54.729, class=0.643, coords=25.930), val_loss=26.129 (conf=1.811, class=0.897, coords=23.422)\n",
      "\n",
      "Epoch 174 :\n",
      "---------------- \n",
      "train_loss=12217.098 (conf=12190.572, class=0.674, coords=25.852), val_loss=25.930 (conf=1.829, class=0.894, coords=23.207)\n",
      "\n",
      "Epoch 175 :\n",
      "---------------- \n",
      "train_loss=32.535 (conf=5.888, class=0.636, coords=26.011), val_loss=25.815 (conf=1.844, class=0.892, coords=23.079)\n",
      "\n",
      "Epoch 176 :\n",
      "---------------- \n",
      "train_loss=48.040 (conf=20.077, class=0.632, coords=27.331), val_loss=25.774 (conf=1.858, class=0.890, coords=23.026)\n",
      "\n",
      "Epoch 177 :\n",
      "---------------- \n",
      "train_loss=40884.676 (conf=40858.051, class=0.658, coords=25.956), val_loss=25.710 (conf=1.873, class=0.888, coords=22.950)\n",
      "\n",
      "Epoch 178 :\n",
      "---------------- \n",
      "train_loss=161.667 (conf=134.788, class=0.649, coords=26.231), val_loss=25.654 (conf=1.885, class=0.890, coords=22.879)\n",
      "\n",
      "Epoch 179 :\n",
      "---------------- \n",
      "train_loss=46.564 (conf=19.867, class=0.668, coords=26.028), val_loss=25.637 (conf=1.894, class=0.891, coords=22.852)\n",
      "\n",
      "Epoch 180 :\n",
      "---------------- \n",
      "train_loss=37.368 (conf=8.939, class=0.663, coords=27.766), val_loss=25.620 (conf=1.904, class=0.888, coords=22.828)\n",
      "\n",
      "Epoch 181 :\n",
      "---------------- \n",
      "train_loss=1097.269 (conf=1073.272, class=0.666, coords=23.332), val_loss=25.590 (conf=1.910, class=0.888, coords=22.792)\n",
      "\n",
      "Epoch 182 :\n",
      "---------------- \n",
      "train_loss=61.187 (conf=33.697, class=0.657, coords=26.833), val_loss=25.578 (conf=1.906, class=0.892, coords=22.781)\n",
      "\n",
      "Epoch 183 :\n",
      "---------------- \n",
      "train_loss=43.848 (conf=18.661, class=0.653, coords=24.534), val_loss=25.550 (conf=1.914, class=0.890, coords=22.746)\n",
      "\n",
      "Epoch 184 :\n",
      "---------------- \n",
      "train_loss=91.450 (conf=67.014, class=0.627, coords=23.809), val_loss=25.514 (conf=1.924, class=0.888, coords=22.703)\n",
      "\n",
      "Epoch 185 :\n",
      "---------------- \n",
      "train_loss=79.938 (conf=54.090, class=0.645, coords=25.204), val_loss=25.500 (conf=1.917, class=0.887, coords=22.695)\n",
      "\n",
      "Epoch 186 :\n",
      "---------------- \n",
      "train_loss=82.172 (conf=57.490, class=0.619, coords=24.063), val_loss=25.512 (conf=1.911, class=0.889, coords=22.712)\n",
      "\n",
      "Epoch 187 :\n",
      "---------------- \n",
      "train_loss=117.405 (conf=93.496, class=0.621, coords=23.287), val_loss=25.450 (conf=1.923, class=0.886, coords=22.641)\n",
      "\n",
      "Epoch 188 :\n",
      "---------------- \n",
      "train_loss=49.118 (conf=23.314, class=0.614, coords=25.190), val_loss=25.453 (conf=1.916, class=0.887, coords=22.650)\n",
      "\n",
      "Epoch 189 :\n",
      "---------------- \n",
      "train_loss=604.906 (conf=579.923, class=0.645, coords=24.339), val_loss=25.478 (conf=1.913, class=0.888, coords=22.678)\n",
      "\n",
      "Epoch 190 :\n",
      "---------------- \n",
      "train_loss=68.986 (conf=41.863, class=0.649, coords=26.474), val_loss=25.470 (conf=1.911, class=0.887, coords=22.672)\n",
      "\n",
      "Epoch 191 :\n",
      "---------------- \n",
      "train_loss=1344.993 (conf=1322.368, class=0.629, coords=21.996), val_loss=25.444 (conf=1.913, class=0.885, coords=22.646)\n",
      "\n",
      "Epoch 192 :\n",
      "---------------- \n",
      "train_loss=1079.924 (conf=1050.667, class=0.668, coords=28.589), val_loss=25.438 (conf=1.906, class=0.887, coords=22.645)\n",
      "\n",
      "Epoch 193 :\n",
      "---------------- \n",
      "train_loss=2988.520 (conf=2960.292, class=0.624, coords=27.604), val_loss=25.424 (conf=1.912, class=0.886, coords=22.627)\n",
      "\n",
      "Epoch 194 :\n",
      "---------------- \n",
      "train_loss=68.609 (conf=42.376, class=0.633, coords=25.600), val_loss=25.400 (conf=1.913, class=0.883, coords=22.605)\n",
      "\n",
      "Epoch 195 :\n",
      "---------------- \n",
      "train_loss=54.747 (conf=29.941, class=0.639, coords=24.167), val_loss=25.368 (conf=1.918, class=0.881, coords=22.570)\n",
      "\n",
      "Epoch 196 :\n",
      "---------------- \n",
      "train_loss=42.819 (conf=17.544, class=0.631, coords=24.645), val_loss=25.419 (conf=1.910, class=0.883, coords=22.626)\n",
      "\n",
      "Epoch 197 :\n",
      "---------------- \n",
      "train_loss=100.245 (conf=72.575, class=0.657, coords=27.013), val_loss=25.434 (conf=1.907, class=0.884, coords=22.643)\n",
      "\n",
      "Epoch 198 :\n",
      "---------------- \n",
      "train_loss=617.070 (conf=589.001, class=0.622, coords=27.448), val_loss=25.425 (conf=1.908, class=0.881, coords=22.636)\n",
      "\n",
      "Epoch 199 :\n",
      "---------------- \n",
      "train_loss=392.299 (conf=367.026, class=0.635, coords=24.639), val_loss=25.382 (conf=1.906, class=0.879, coords=22.597)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'weight/training_1_epoch_final_666.h5'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    epoch_sub_loss = []\n",
    "    epoch_val_loss = []\n",
    "    epoch_val_sub_loss = []\n",
    "    print('\\nEpoch {} :'.format(epoch))\n",
    "\n",
    "    # train\n",
    "    for bs_idx, (x, y) in enumerate(dataset_train):\n",
    "        x, detector_mask, y_true_anchor_boxes, y_true_class_hot, y_true_boxes_all = transform(x, y, cfg)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x, training=True)\n",
    "            loss, sub_loss = yolo_loss(detector_mask, y_true_anchor_boxes, y_true_class_hot, y_true_boxes_all, y_pred, cfg)\n",
    "            _loss = loss * 0.01\n",
    "        grads = tape.gradient(_loss, train_vars)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, train_vars))\n",
    "        epoch_loss.append(loss)\n",
    "        epoch_sub_loss.append(sub_loss)\n",
    "        if (bs_idx + 1) % (math.ceil(num_iterations / n_progress)) == 0:\n",
    "            print('-', end='')\n",
    "        if (bs_idx + 1) == num_iterations:\n",
    "            break\n",
    "\n",
    "    # val\n",
    "    for bs_idx, (x,y) in enumerate(dataset_val):\n",
    "        x, detector_mask, y_true_anchor_boxes, y_true_class_hot, y_true_boxes_all = transform(x, y, cfg)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x, training=False)\n",
    "            loss, sub_loss = yolo_loss(detector_mask, y_true_anchor_boxes, y_true_class_hot, y_true_boxes_all, y_pred, cfg)\n",
    "        epoch_val_loss.append(loss)\n",
    "        epoch_val_sub_loss.append(sub_loss)\n",
    "        print('-', end='')\n",
    "        if (bs_idx+1)==1:\n",
    "            break\n",
    "\n",
    "    # 记录\n",
    "    loss_avg = np.mean(np.array(epoch_loss))\n",
    "    sub_loss_avg = np.mean(np.array(epoch_sub_loss), axis=0)\n",
    "    val_loss_avg = np.mean(np.array(epoch_val_loss))\n",
    "    val_sub_loss_avg = np.mean(np.array(epoch_val_sub_loss), axis=0)\n",
    "\n",
    "    log_loss(loss_avg, val_loss_avg, step=epoch)\n",
    "    train_loss_history.append(loss_avg)\n",
    "    val_loss_history.append(val_loss_avg)\n",
    "\n",
    "    if loss_avg < best_val_loss:\n",
    "        print('\\nfind better model for train')\n",
    "        best_model_path = save_best_weights(model, train_name+'_epoch%d' % epoch, loss_avg)\n",
    "        best_val_loss = loss_avg\n",
    "    print(' \\ntrain_loss={:.3f} (conf={:.3f}, class={:.3f}, coords={:.3f}), val_loss={:.3f} (conf={:.3f}, class={:.3f}, coords={:.3f})'.format(\n",
    "            loss_avg, sub_loss_avg[0], sub_loss_avg[1], sub_loss_avg[2], val_loss_avg, val_sub_loss_avg[0], val_sub_loss_avg[1], val_sub_loss_avg[2]))\n",
    "save_best_weights(model, train_name+'_epoch_final', 666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.842194,
     "end_time": "2021-01-15T07:05:21.165176",
     "exception": false,
     "start_time": "2021-01-15T07:05:20.322982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T07:05:22.880358Z",
     "iopub.status.busy": "2021-01-15T07:05:22.870064Z",
     "iopub.status.idle": "2021-01-15T07:05:24.473424Z",
     "shell.execute_reply": "2021-01-15T07:05:24.472936Z"
    },
    "papermill": {
     "duration": 2.461396,
     "end_time": "2021-01-15T07:05:24.473526",
     "exception": false,
     "start_time": "2021-01-15T07:05:22.012130",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AxesSubplot' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-15-2d109011c713>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     90\u001B[0m     \u001B[0miou_threshold\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0.45\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     91\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 92\u001B[0;31m     \u001B[0mdisplay_img\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscore_threshold\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0miou_threshold\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcfg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     93\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mbs_idx\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     94\u001B[0m         \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-15-2d109011c713>\u001B[0m in \u001B[0;36mdisplay_img\u001B[0;34m(input_img, model, score_threshold, iou_threshold, config)\u001B[0m\n\u001B[1;32m     81\u001B[0m                       boxes[:,3] - boxes[:,1]),\n\u001B[1;32m     82\u001B[0m                       axis=-1) # x1, y1, x2, y2 ==> cx, cy, w, h\n\u001B[0;32m---> 83\u001B[0;31m     \u001B[0mplot_img\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_img\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_boxes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclasses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscores\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     84\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0m_boxes\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-15-2d109011c713>\u001B[0m in \u001B[0;36mplot_img\u001B[0;34m(img, boxes, labels, scores, config)\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0mscores\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mscores\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 18\u001B[0;31m     \u001B[0max\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     19\u001B[0m     \u001B[0mimg_h\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimg_w\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m     \u001B[0mcolors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'r'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'orange'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'g'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'b'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'pink'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'purple'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'AxesSubplot' object has no attribute 'show'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_img(img, boxes, labels, scores, config):\n",
    "    \"\"\"\n",
    "    画图\n",
    "    :param img: 图片\n",
    "    :param boxes: 物体位置box\n",
    "    :param labels: 标签\n",
    "    :param scores: 得分\n",
    "    :param config: 配置\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 画图\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    img = img.numpy()\n",
    "    boxes = boxes.numpy().reshape(-1, 4)\n",
    "    labels = labels.numpy()\n",
    "    scores = scores.numpy()\n",
    "\n",
    "    ax.imshow(img)\n",
    "    img_h, img_w = img.shape[0: 2]\n",
    "    colors = ['r', 'orange', 'g', 'b', 'pink', 'purple']\n",
    "    count = 0\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        cx, cy, w, h = box\n",
    "        if w * h <= 0:\n",
    "            continue\n",
    "        count += 1\n",
    "        cx = cx / config.GRID_W * img_w\n",
    "        cy = cy/config.GRID_H * img_h\n",
    "        w = w/config.GRID_W * img_w\n",
    "        h = h/config.GRID_H * img_h\n",
    "        name = cfg.VOC_LABEL_NAME[label+1]\n",
    "        ax.scatter(cx, cy, s=10, c='yellow')\n",
    "        text = ' No:%d' % count + '_'+ name + ' %.3f' % score\n",
    "        ax.text(cx-w/2, cy+h/2, text, fontdict={'size':15, 'color':colors[count-1]})\n",
    "        rect = patches.Rectangle((cx-w/2,cy-h/2), w, h, edgecolor=colors[count-1], linewidth=3.0, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "def display_img(input_img, model, score_threshold, iou_threshold, config):\n",
    "    \"\"\"\n",
    "    把标签展示到图片上\n",
    "    :param input_img: 测试图片\n",
    "    :param model: yolo模型\n",
    "    :param score_threshold: 根据box_conf * box_class_prob筛选boxes\n",
    "    :param iou_threshold: 对boxes做NMS时的阈值\n",
    "    :param config: 配置\n",
    "    :return: box位置\n",
    "    \"\"\"\n",
    "    y_pred = model.predict_on_batch(tf.expand_dims(input_img, 0))\n",
    "    cell_coord_x = tf.cast(tf.reshape(tf.tile(tf.range(config.GRID_W), [config.GRID_H]), (1, config.GRID_H, config.GRID_W, 1, 1)), tf.float32)\n",
    "    cell_coord_y = tf.transpose(cell_coord_x, (0,2,1,3,4))\n",
    "    cell_coords = tf.tile(tf.concat([cell_coord_x, cell_coord_y], -1), [y_pred.shape[0], 1, 1, 5, 1])\n",
    "    anchors = config.ANCHORS\n",
    "\n",
    "    pred_xy = K.sigmoid(y_pred[:,:,:,:,0:2])\n",
    "    pred_xy = pred_xy + cell_coords\n",
    "    pred_wh = K.exp(y_pred[:,:,:,:,2:4]) * anchors\n",
    "\n",
    "    box_conf = K.sigmoid(y_pred[:,:,:,:,4:5])\n",
    "    box_class_prob = K.softmax(y_pred[:,:,:,:,5:])\n",
    "    box_xy1 = pred_xy - 0.5 * pred_wh\n",
    "    box_xy2 = pred_xy + 0.5 * pred_wh\n",
    "    boxes = K.concatenate((box_xy1, box_xy2), axis=-1)\n",
    "\n",
    "    box_scores = box_conf * box_class_prob\n",
    "\n",
    "    box_classes = K.argmax(box_scores, axis=-1) # 最好的分数的 index\n",
    "    box_class_scores = K.max(box_scores, axis=-1) # 最好的分数\n",
    "    prediction_mask = box_class_scores >= score_threshold\n",
    "    boxes = tf.boolean_mask(boxes, prediction_mask)\n",
    "    scores = tf.boolean_mask(box_class_scores, prediction_mask)\n",
    "    classes = tf.boolean_mask(box_classes, prediction_mask)\n",
    "\n",
    "    # NMS（非极大值抑制）\n",
    "    selected_idx = tf.image.non_max_suppression(boxes, scores, 50, iou_threshold=iou_threshold)\n",
    "    boxes = K.gather(boxes, selected_idx)  # [n, 4]\n",
    "    scores = K.gather(scores, selected_idx)  # [n,]\n",
    "    classes = K.gather(classes, selected_idx)  # [n,]\n",
    "    _boxes = K.stack((0.5*(boxes[:,0] + boxes[:,2]),\n",
    "                      0.5*(boxes[:,1] + boxes[:,3]),\n",
    "                      boxes[:,2] - boxes[:,0],\n",
    "                      boxes[:,3] - boxes[:,1]),\n",
    "                      axis=-1) # x1, y1, x2, y2 ==> cx, cy, w, h\n",
    "    plot_img(input_img, _boxes, classes, scores, config)\n",
    "    return _boxes\n",
    "\n",
    "model.load_weights(best_model_path)\n",
    "for bs_idx, (x,y) in enumerate(dataset_val):\n",
    "    x, detector_mask, y_true_anchor_boxes, y_true_class_hot, y_true_boxes_all = transform(x, y, cfg)\n",
    "    score_threshold = 0.5\n",
    "    iou_threshold = 0.45\n",
    "\n",
    "    display_img(x[0], model, score_threshold, iou_threshold, cfg)\n",
    "    if (bs_idx + 1) == 10:\n",
    "        break\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "ax.plot(train_loss_history[10:])\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1372.504957,
   "end_time": "2021-01-15T07:05:27.424086",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-15T06:42:34.919129",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}